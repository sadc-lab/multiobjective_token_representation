{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fd8e54a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/xval2_venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from transformers import CamembertForTokenClassification_xval, CamembertForMaskedLM_xval, AutoTokenizer, AutoConfig\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from LabelClassif.data import plot_confusion_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e1204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3f465b-8753-4efa-9120-ad2ef9158b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stats(data):\n",
    "    average = np.mean(data, axis=0)\n",
    "    std =  np.mean((data-average)**2, axis=0)\n",
    "    std = np.sqrt(std)\n",
    "    return average, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bff1dee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#decode batch of tokenized sentences into sentences\n",
    "def decode_properly(batch_input_ids, tokenizer):\n",
    "    sentences = []\n",
    "    for input_ids in batch_input_ids:\n",
    "        sentences.append(tokenizer.decode(input_ids, skip_special_tokens= True))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a554605a-b99a-420a-8ee9-3771b0184d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomizedDataset(Dataset):\n",
    "    def __init__(self, dataset_list):\n",
    "        self.files = dataset_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.files[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec9c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('../camembert-bio-model')\n",
    "tokenizer.add_tokens(\"NUM\")\n",
    "tokenizer.save_pretrained(\"../checkpoints/tokenizer_xval/\")\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('../checkpoints/tokenizer_xval/')\n",
    "\n",
    "keyword_id = 32005\n",
    "len_tokenizer = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ee3f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_exponents = -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bd197d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_precision_recall_score(y_true, y_pred, nb_labels):\n",
    "    matrix = confusion_matrix(y_true, y_pred, labels = list(range(nb_labels)))\n",
    "    precision = np.zeros(nb_labels)\n",
    "    recall = np.zeros(nb_labels)\n",
    "    sum_cols = np.sum(matrix, axis = 0)\n",
    "    sum_lines = np.sum(matrix, axis = 1)\n",
    "    acc = 0\n",
    "    for i in range(nb_labels):\n",
    "        sum_col_i = sum_cols[i] + 1e-7\n",
    "        sum_line_i = sum_lines[i] + 1e-7\n",
    "        precision[i] = matrix[i, i]/sum_col_i\n",
    "        recall[i] = matrix[i, i]/sum_line_i\n",
    "        acc += matrix[i, i]\n",
    "    return precision, recall, acc/sum(sum_cols)\n",
    "\n",
    "def custom_f1_score(y_true, y_pred, nb_labels, average = None):\n",
    "    #average = None or \"macro\"\n",
    "    precision, recall, acc = custom_precision_recall_score(y_true, y_pred, nb_labels)\n",
    "    scores = 2*precision*recall/(precision + recall + 1e-7)\n",
    "    if average == None:\n",
    "        return scores\n",
    "    else: \n",
    "        return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c23c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSLELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return self.mse(torch.log(pred + 1), torch.log(actual + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2df9086a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model_name, optim, lr, weight_decay, T_0=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters('model_name', 'optim', 'lr', 'weight_decay', 'T_0') \n",
    "        self.optim = optim   \n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.T_0 = T_0\n",
    "        # We apply the multitask optimization approach as explained in Kendall et. al. 2017\n",
    "        self.log_sigma_2 = torch.nn.Parameter(torch.zeros(3)) #We have three losses\n",
    "        self.name = model_name\n",
    "        if self.name == 'XVAL2_BertForMaskedLM':\n",
    "            self.model = CamembertForMaskedLM_xval.from_pretrained(model_directory)\n",
    "            self.num_labels = len_tokenizer #self.model.lm_head.decoder.out_features\n",
    "        elif self.name == 'XVAL2_BertForTokClassif':\n",
    "            self.model = CamembertForTokenClassification_xval.from_pretrained(model_directory)\n",
    "            self.num_labels = self.model.num_labels\n",
    "        else:\n",
    "            raise ValueError('model name:' + model_name + 'is unknown')\n",
    "            \n",
    "        self.model.resize_token_embeddings(len_tokenizer)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        return self.model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            label_inputs = batch[\"label_inputs\"],\n",
    "            h_num = batch[\"h_nums\"],\n",
    "            attention_mask=batch[\"attention_mask\"]\n",
    "            )\n",
    "        \n",
    "    def training_step(self, batch):\n",
    "        out = self.forward(batch)\n",
    "\n",
    "        # -------- MASKED --------\n",
    "        if self.name != 'XVAL2_BertForMaskedLM':\n",
    "            logits = out.logits\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()#weight=28000/torch.tensor([28000, 21, 80, 57, 143, 130, 41, 58], device='cuda'))\n",
    "            loss = loss_fn(logits.view(-1, self.num_labels), batch[\"labels\"].view(-1))\n",
    "\n",
    "        else:\n",
    "            pred_logits, num_logits = out.logits\n",
    "            significand_logits, exponent_logits = num_logits\n",
    "\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss_1 = loss_fn(pred_logits.view(-1, self.num_labels), batch[\"labels\"].view(-1))\n",
    "            \n",
    "            #selecting the tokens to be classified by NUM head\n",
    "            condition = torch.flatten((batch[\"labels\"].view(-1)==keyword_id).nonzero())\n",
    "            #print(batch[\"labels\"].view(-1)[condition])\n",
    "            #print(batch[\"h_nums\"].view(-1)[condition])\n",
    "            loss_2 = loss_fn((exponent_logits.view(-1, exponent_logits.size(2)))[condition], (batch[\"exponents\"].view(-1))[condition])\n",
    "            \n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            loss_3 = loss_fn((significand_logits.view(-1))[condition], (batch[\"significands\"].view(-1))[condition])\n",
    "            #loss = loss_1 + loss_2 + loss_3\n",
    "            \n",
    "            weights = torch.exp(-self.log_sigma_2)\n",
    "            loss =  weights[0]*loss_1 + weights[1]*loss_2 + .5*weights[2]*loss_3 + .5*self.log_sigma_2.sum()\n",
    "            \n",
    "        # ------ END MASKED ------\n",
    "\n",
    "        self.log(\"train/loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        out = self.forward(batch)\n",
    "        \n",
    "        if self.name != 'XVAL2_BertForMaskedLM':\n",
    "            preds = torch.max(out.logits, -1).indices\n",
    "            f1 = custom_f1_score(batch[\"labels\"].view(-1).cpu().numpy(), preds.view(-1).cpu().numpy(),\n",
    "                 self.num_labels, average = \"macro\")\n",
    "            self.log(\"valid/f1\", f1, prog_bar=True, on_step=False, on_epoch=True)\n",
    "            \n",
    "        else:\n",
    "            pred_logits, num_logits = out.logits\n",
    "            significand_logits, exponent_logits = num_logits\n",
    "\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss_1 = loss_fn(pred_logits.view(-1, self.num_labels), batch[\"labels\"].view(-1))\n",
    "            \n",
    "            #selecting the tokens to be classified by NUM head\n",
    "            condition = torch.flatten((batch[\"labels\"].view(-1)==keyword_id).nonzero())\n",
    "            loss_2 = loss_fn((exponent_logits.view(-1, exponent_logits.size(2)))[condition], (batch[\"exponents\"].view(-1))[condition])\n",
    "            \n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            loss_3 = loss_fn((significand_logits.view(-1))[condition], (batch[\"significands\"].view(-1))[condition])\n",
    "            #loss = loss_1 + loss_2 + loss_3\n",
    "            \n",
    "            weights = torch.exp(-self.log_sigma_2)\n",
    "            loss =  weights[0]*loss_1 + weights[1]*loss_2 + .5*weights[2]*loss_3 + .5*self.log_sigma_2.sum()\n",
    "            self.log(\"valid/loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "            \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        \"\"\"La fonction predict step facilite la prédiction de données. Elle est \n",
    "        similaire à `validation_step`, sans le calcul des métriques.\n",
    "        \"\"\"\n",
    "        out = self.forward(batch)\n",
    "        if self.name != 'XVAL2_BertForMaskedLM':\n",
    "            return torch.max(out.logits, -1).indices\n",
    "        \n",
    "        batch_size, sentence_length = batch[\"input_ids\"].size()\n",
    "        pred_logits, num_logits = out.logits\n",
    "        significand_logits, exponent_logits = num_logits\n",
    "\n",
    "        texts = torch.max(pred_logits.view(-1, self.num_labels), -1).indices\n",
    "        exponents = torch.max(exponent_logits.view(-1, exponent_logits.size(2)), -1).indices\n",
    "        numbers = significance_logits.view(-1) * (exponents + min_exponents)\n",
    "        # we replace the masked tokens corresponding to numbers by the predicted numbers\n",
    "        condition = torch.flatten((texts==keyword_id).nonzero())\n",
    "            \n",
    "        texts[condition] = numbers[condition]\n",
    "        return texts.view(batch_size, sentence_length)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.optim == 'AdamW':\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                        self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "                    )\n",
    "            optimizer.add_param_group({'params': self.log_sigma_2})\n",
    "            \n",
    "        elif self.optim == 'SGD':\n",
    "            optimizer = torch.optim.SGD(\n",
    "                            self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "                        )\n",
    "            optimizer.add_param_group({'params': self.log_sigma_2})\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Optim name: ' + self.optim + ' is unknown')\n",
    "        \n",
    "        if self.T_0 is None:\n",
    "            return optimizer\n",
    "        else:\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, self.T_0)\n",
    "            return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fd81f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#key words for the sequence classification task\n",
    "key_words = np.array([['mot', 'patient', 'historique'],\n",
    "                      ['fraction', 'ejection', 'raccourcissement'],\n",
    "                      ['cardiaque', 'coeur', 'fréquence'],\n",
    "                      ['diamètre', 'pulmonaire', 'artère'], \n",
    "                      ['oxygène', 'O2', 'saturation'],\n",
    "                      ['apgar', 'minute', 'nombre'],\n",
    "                      ['gradient', 'pulmonaire', 'ventricule'],\n",
    "                      ['cia', 'civ', 'inter']\n",
    "                     ])\n",
    "num_labels = 8 #['O', 'Cp', 'FC', 'D', 'SO2', 'AGPR', 'G', 'CI']\n",
    "num_words_per_label = 3\n",
    "label_inputs = torch.zeros((num_labels, num_words_per_label), dtype = int)\n",
    "\n",
    "for label in range(num_labels):\n",
    "    for i in range(num_words_per_label):\n",
    "        label_inputs[label, i] = tokenizer.encode(key_words[label, i],\n",
    "                   add_special_tokens = False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea11036",
   "metadata": {},
   "source": [
    "# Training Langage Modeling with the whole unlabeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = '../camembert-bio-model'\n",
    "#checkpoint_path = '../checkpoints/xval2_camembert_mlm/epoch=130-step=96678.ckpt' #trained (with logsigma) on AdamW, lr=3e-5\n",
    "#checkpoint_path = '../checkpoints/xval2_camembert_mlm/epoch=12-step=9594.ckpt' #trained (with logsigma) on AdamW, lr=3e-5 then SGD\n",
    "#checkpoint_path = '../checkpoints/xval2_camembert_mlm/epoch=61-step=45756.ckpt' #LESA XVAL trained (with logsigma) on AdamW, lr=3e-5\n",
    "checkpoint_path = '../checkpoints/xval2_camembert_mlm/epoch=9-step=7380.ckpt' #LESA XVAL trained (with logsigma) on AdamW, lr=3e-5 then SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fbbd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../sadcsip/xval_mlm_val\", \"rb\") as fp:   # Unpickling\n",
    "   mlm_val_ds = pickle.load(fp)\n",
    "with open(\"../sadcsip/xval_mlm_train\", \"rb\") as fp:   # Unpickling\n",
    "   mlm_train_ds = pickle.load(fp)\n",
    "\n",
    "mlm_val_ds = CustomizedDataset(mlm_val_ds)\n",
    "mlm_train_ds = CustomizedDataset(mlm_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af2f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer):\n",
    "    text = [example[\"tokens\"] for example in examples]\n",
    "    tokenized_inputs = tokenizer(text, \n",
    "                                 padding=\"longest\", truncation=True, return_tensors=\"pt\",\n",
    "                                 is_split_into_words=True)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs['input_ids'].detach().clone()\n",
    "    \n",
    "    #aligning h_nums with the tokens\n",
    "    h_nums = []\n",
    "    significands = []\n",
    "    exponents = []\n",
    "    for i in range(len(examples)):\n",
    "        h_num = examples[i][\"h_nums\"]\n",
    "        significand = examples[i][\"significands\"]\n",
    "        exponent = examples[i][\"exponents\"]\n",
    "\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index = i)  # Map tokens to their respective word.\n",
    "        h_num_extended = []\n",
    "        significands_extended = []\n",
    "        exponents_extended = []\n",
    "        for word_idx in word_ids:  \n",
    "            if word_idx is None: # Set the special tokens to 1.\n",
    "                h_num_extended.append(1)\n",
    "                significands_extended.append(1.0)\n",
    "                exponents_extended.append(-min_exponents)\n",
    "            elif text[i][word_idx] == \"NUM\":  # number is encountered.\n",
    "                h_num_extended.append(h_num[word_idx])\n",
    "                significands_extended.append(significand[word_idx])\n",
    "                exponents_extended.append(exponent[word_idx])\n",
    "            else:\n",
    "                h_num_extended.append(1)\n",
    "                significands_extended.append(1.0)\n",
    "                exponents_extended.append(-min_exponents)\n",
    "            \n",
    "        h_nums.append(h_num_extended)\n",
    "        significands.append(significands_extended)\n",
    "        exponents.append(exponents_extended)\n",
    "\n",
    "    h_nums = torch.tensor(h_nums, dtype=torch.float32)\n",
    "    significands = torch.tensor(significands, dtype=torch.float32)\n",
    "    exponents = torch.tensor(exponents, dtype=tokenized_inputs[\"labels\"].dtype)\n",
    "    \n",
    "    tokenized_inputs[\"h_nums\"] = h_nums\n",
    "    tokenized_inputs[\"significands\"] = significands\n",
    "    tokenized_inputs[\"exponents\"] = exponents\n",
    "    \n",
    "    # create random array of floats with equal dimensions to input_ids tensor\n",
    "    rand = torch.rand(tokenized_inputs['input_ids'].shape)\n",
    "    # create mask array while avoiding CLS, SEP and PAD\n",
    "    mask_arr = (rand < 0.15) * (tokenized_inputs['input_ids'] != tokenizer.bos_token_id) * \\\n",
    "           (tokenized_inputs['input_ids'] != tokenizer.sep_token_id) * (tokenized_inputs['input_ids'] != tokenizer.pad_token_id)\n",
    "    \n",
    "    selection = []\n",
    "    for i in range(tokenized_inputs['input_ids'].shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "    #applying the mask\n",
    "    for i in range(tokenized_inputs['input_ids'].shape[0]):\n",
    "        tokenized_inputs[\"input_ids\"][i, selection[i]] = tokenizer.mask_token_id\n",
    "        tokenized_inputs[\"h_nums\"][i, selection[i]] = 1.\n",
    "    \n",
    "    tokenized_inputs[\"label_inputs\"] = label_inputs\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc374f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "examples = [{\"tokens\":[\"le\", \"ph\", \"=\", \"nombre\", \".\"], \"h_nums\":[1, 1, 1, 7, 1]}, \n",
    "            {\"tokens\":[\"TA\", \"nombre\", \"mmgh\", \".\"], \"h_nums\":[1, 120, 1, 1]}]\n",
    "tokenize_and_align_labels(examples, tokenizer)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7170b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#camembert_mlm = LightningModel(model_name = 'XVAL2_BertForMaskedLM', optim='AdamW', lr=3e-5, weight_decay=0.01)\n",
    "camembert_mlm = LightningModel.load_from_checkpoint(checkpoint_path=checkpoint_path)#, optim='SGD', lr=1e-7, weight_decay=0.01, T_0=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "it= iter(val_dataloader);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2b6912",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(it)\n",
    "batch = {k:v.cuda() for k,v in batch.items()}\n",
    "out = camembert_mlm.model(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        label_inputs = batch[\"label_inputs\"],\n",
    "        h_num = batch[\"h_nums\"],\n",
    "        attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "pred_logits, num_logits = out.logits\n",
    "significand_logits, exponent_logits = num_logits\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_1 = loss_fn(pred_logits.view(-1, camembert_mlm.num_labels), batch[\"labels\"].view(-1))\n",
    "            \n",
    "#selecting the tokens to be classified by NUM head\n",
    "condition = torch.flatten((batch[\"labels\"].view(-1)==keyword_id).nonzero())\n",
    "#print(batch[\"labels\"].view(-1)[condition])\n",
    "#print(batch[\"h_nums\"].view(-1)[condition])\n",
    "loss_2 = loss_fn((exponent_logits.view(-1, exponent_logits.size(2)))[condition], (batch[\"exponents\"].view(-1))[condition])\n",
    "            \n",
    "loss_fn = torch.nn.MSELoss()\n",
    "loss_3 = loss_fn((significand_logits.view(-1))[condition], (batch[\"significands\"].view(-1))[condition])\n",
    "loss = loss_1 + loss_2 + loss_3\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075249ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_1.item(), loss_2.item(), loss_3.item(), loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6151efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(significand_logits.view(-1))[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65960db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(batch[\"significands\"].view(-1))[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be7198",
   "metadata": {},
   "outputs": [],
   "source": [
    "camembert_mlm.log_sigma_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9824f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(exponent_logits.view(-1, exponent_logits.size(2)), -1).indices[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(batch[\"exponents\"].view(-1))[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81857b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7bf515",
   "metadata": {},
   "outputs": [],
   "source": [
    "camembert_mlm_checkpoint = pl.callbacks.ModelCheckpoint(dirpath = '../checkpoints/xval2_camembert_mlm',\n",
    "                                                      monitor=\"valid/loss\", mode=\"min\")\n",
    "\n",
    "camembert_mlm_trainer = pl.Trainer(\n",
    "    #max_epochs=100,\n",
    "    callbacks=[\n",
    "        pl.callbacks.EarlyStopping(monitor=\"valid/loss\", patience=10, mode=\"min\"),\n",
    "        camembert_mlm_checkpoint,\n",
    "    ],\n",
    "    #detect_anomaly=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    mlm_train_ds, \n",
    "    batch_size=24, \n",
    "    shuffle=True, \n",
    "    collate_fn=functools.partial(tokenize_and_align_labels, tokenizer=tokenizer),\n",
    "    num_workers=8\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    mlm_val_ds,\n",
    "    batch_size=24, \n",
    "    shuffle=False, \n",
    "    collate_fn=functools.partial(tokenize_and_align_labels, tokenizer=tokenizer),\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7583ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "camembert_mlm_trainer.fit(camembert_mlm, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c924bccc",
   "metadata": {},
   "source": [
    "# Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38d7d017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_directory = \"../camembert-bio-model\"\n",
    "\n",
    "#checkpoint_path = '../checkpoints/xval2_camembert_mlm/epoch=12-step=9594.ckpt' #trained (with logsigma) on AdamW, lr=3e-5 then SGD\n",
    "checkpoint_path = '../checkpoints/xval2_camembert_mlm/epoch=9-step=7380.ckpt' #LESA XVAL trained (with logsigma) on AdamW, lr=3e-5 then SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76272b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"../sadcsip/xval_test\", \"rb\") as fp:   # Unpickling\n",
    "   test_ds = pickle.load(fp)\n",
    "\n",
    "with open(\"../sadcsip/xval_val\", \"rb\") as fp:   # Unpickling\n",
    "   val_ds = pickle.load(fp)\n",
    "with open(\"../sadcsip/xval_train\", \"rb\") as fp:   # Unpickling\n",
    "   train_ds = pickle.load(fp)\n",
    "\n",
    "test_ds = CustomizedDataset(test_ds)\n",
    "val_ds = CustomizedDataset(val_ds)\n",
    "train_ds = CustomizedDataset(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "766c46ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer):\n",
    "    text = [example[\"tokens\"] for example in examples]\n",
    "    tokenized_inputs = tokenizer(text, \n",
    "                                 padding=\"longest\", truncation=True, return_tensors=\"pt\",\n",
    "                                 is_split_into_words=True)\n",
    "    #aligning h_nums with the tokens\n",
    "    h_nums = []\n",
    "    for i in range(len(examples)):\n",
    "        h_num = examples[i][\"h_nums\"]\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index = i)  # Map tokens to their respective word.\n",
    "        h_num_extended = []\n",
    "        for word_idx in word_ids:  \n",
    "            if word_idx is None: # Set the special tokens to 1.\n",
    "                h_num_extended.append(1)\n",
    "            elif text[i][word_idx] == \"NUM\":  # number is encountered.\n",
    "                h_num_extended.append(h_num[word_idx])\n",
    "            else:\n",
    "                h_num_extended.append(1)\n",
    "        h_nums.append(h_num_extended)\n",
    "    h_nums = torch.tensor(h_nums, dtype=torch.float32)\n",
    "    tokenized_inputs[\"h_nums\"] = h_nums\n",
    "    \n",
    "    labels = []\n",
    "    for i in range(len(examples)):\n",
    "        label = examples[i][\"classes\"]\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index = i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    tokenized_inputs[\"label_inputs\"] = label_inputs\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fccced1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b665df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(\n",
    "        val_ds, \n",
    "        batch_size=24, \n",
    "        shuffle=False,\n",
    "        num_workers = 8,\n",
    "        collate_fn=functools.partial(tokenize_and_align_labels, tokenizer=tokenizer)\n",
    "    )\n",
    "    #next(iter(val_dataloader))\n",
    "test_dataloader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=24,\n",
    "    shuffle=False,\n",
    "    num_workers = 8,\n",
    "    collate_fn=functools.partial(tokenize_and_align_labels, tokenizer=tokenizer)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "484fdaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_xval: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_xval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_xval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_xval were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lomboa00/.conda/envs/xval2_venv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | model | CamembertForTokenClassification_xval | 110 M \n",
      "---------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.159   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/xval2_venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 31/31 [00:12<00:00,  2.55it/s, v_num=786, valid/f1=0.239, train/loss=0.220]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_xval: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_xval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_xval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_xval were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lomboa00/.conda/envs/xval2_venv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/lomboa00/.conda/envs/xval2_venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/lomboa00/checkpoints/xval2_camembert_tok_lesa exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | model | CamembertForTokenClassification_xval | 110 M \n",
      "---------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.159   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/xval2_venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 31/31 [00:12<00:00,  2.43it/s, v_num=787, valid/f1=0.200, train/loss=0.246]\n"
     ]
    }
   ],
   "source": [
    "seeds = [10, 100, 1000, 10000, 100000, 12, 123, 1234, 12345, 123456]\n",
    "for rep in range(2):#len(seeds)):\n",
    "    torch.manual_seed(seeds[rep])\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds, \n",
    "        batch_size=24, \n",
    "        shuffle=True, \n",
    "        num_workers = 8,\n",
    "        collate_fn=functools.partial(tokenize_and_align_labels, tokenizer=tokenizer)\n",
    "    )\n",
    "    camembert_mlm = LightningModel.load_from_checkpoint(checkpoint_path=checkpoint_path)\n",
    "    \n",
    "    camembert_tok = LightningModel(\"XVAL2_BertForTokClassif\", optim='AdamW', lr=3e-5, weight_decay=0.01)\n",
    "    #adding the lesa_bert model trained previously\n",
    "    camembert_tok.model.roberta = camembert_mlm.model.roberta\n",
    "\n",
    "    #camembert_tok = LightningModel.load_from_checkpoint(checkpoint_path= checkpoint_path)\n",
    "    #for training\n",
    "    model_checkpoint = pl.callbacks.ModelCheckpoint(#dirpath = '../checkpoints/xval2_camembert_tok',\n",
    "                                                    dirpath = '../checkpoints/xval2_camembert_tok_lesa',\n",
    "                                                    monitor=\"valid/f1\", mode=\"max\")\n",
    "\n",
    "    camembert_trainer = pl.Trainer(\n",
    "        #max_epochs=100,\n",
    "        callbacks=[\n",
    "            pl.callbacks.EarlyStopping(monitor=\"valid/f1\", patience=5, mode=\"max\"),\n",
    "            model_checkpoint,\n",
    "        ]\n",
    "    )\n",
    "    camembert_trainer.fit(camembert_tok, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14bd2818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_xval: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_xval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_xval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_xval were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lomboa00/.conda/envs/xval2_venv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | model | CamembertForTokenClassification_xval | 110 M \n",
      "---------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.159   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/xval2_venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 31/31 [00:12<00:00,  2.56it/s, v_num=784, valid/f1=0.240, train/loss=0.593]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_xval: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_xval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_xval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_xval were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/lomboa00/.conda/envs/xval2_venv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/lomboa00/.conda/envs/xval2_venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/lomboa00/checkpoints/xval2_camembert_tok_lesa_tuned exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | model | CamembertForTokenClassification_xval | 110 M \n",
      "---------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.159   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/.conda/envs/xval2_venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (31) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 31/31 [00:12<00:00,  2.45it/s, v_num=785, valid/f1=0.256, train/loss=0.146]\n"
     ]
    }
   ],
   "source": [
    "#path = '../checkpoints/xval2_camembert_tok'\n",
    "path = '../checkpoints/xval2_camembert_tok_lesa'\n",
    "rep=0\n",
    "seeds = [10, 100, 1000, 10000, 100000, 12, 123, 1234, 12345, 123456]\n",
    "\n",
    "for filename in os.listdir(path):    \n",
    "    checkpoint_path = os.path.join(path, filename)\n",
    "    torch.manual_seed(seeds[rep])\n",
    "    rep += 1\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds, \n",
    "        batch_size=24, \n",
    "        shuffle=True, \n",
    "        num_workers = 8,\n",
    "        collate_fn=functools.partial(tokenize_and_align_labels, tokenizer=tokenizer)\n",
    "    )\n",
    "    camembert_tok = LightningModel.load_from_checkpoint(checkpoint_path=checkpoint_path, optim='SGD',\n",
    "             lr=1e-6, weight_decay=0.01)\n",
    "    #for training\n",
    "    model_checkpoint = pl.callbacks.ModelCheckpoint(#dirpath = '../checkpoints/xval2_camembert_tok_tuned',\n",
    "                                                    dirpath = '../checkpoints/xval2_camembert_tok_lesa_tuned',\n",
    "                                                    monitor=\"valid/f1\", mode=\"max\")\n",
    "\n",
    "    camembert_trainer = pl.Trainer(\n",
    "        #max_epochs=100,\n",
    "        callbacks=[\n",
    "            pl.callbacks.EarlyStopping(monitor=\"valid/f1\", patience=5, mode=\"max\"),\n",
    "            model_checkpoint,\n",
    "        ]\n",
    "    )\n",
    "    camembert_trainer.fit(camembert_tok, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59561d1",
   "metadata": {},
   "source": [
    "# Predicting with Token Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8643f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "camembert_tok = LightningModel.load_from_checkpoint(checkpoint_path= '../checkpoints/xval_camembert_tok/epoch=33-step=2108.ckpt', optim='AdamW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356bf9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['O', 'Cp', 'FC', 'D', 'SO2', 'AGPR', 'G', 'CI']\n",
    "nb_labels = len(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a358c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "#for prediction\n",
    "prediction_trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    callbacks=[\n",
    "        pl.callbacks.EarlyStopping(monitor=\"valid/f1\", patience=4, mode=\"max\"),\n",
    "    ]\n",
    ")\n",
    "camembert_preds = prediction_trainer.predict(camembert_tok, dataloaders=test_dataloader)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f043e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(camembert_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4c46f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "camembert_preds = [batch_preds.view(-1) for batch_preds in camembert_preds]\n",
    "camembert_preds = torch.cat(camembert_preds, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b4730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# collecting the labels\n",
    "labels = []\n",
    "it = iter(test_dataloader)\n",
    "exit = False\n",
    "while not exit:\n",
    "    try:\n",
    "        # Samples the batch\n",
    "        tokenized_batch = next(it)\n",
    "        label = tokenized_batch['labels']\n",
    "        labels.append(label.view(-1))\n",
    "    except StopIteration:\n",
    "        exit = True\n",
    "labels = torch.cat(labels, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c0d36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(labels, camembert_preds, label_names, 'camembert_tok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f59ab16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision, recall, _ = custom_precision_recall_score(labels, camembert_preds, nb_labels)\n",
    "print('precision =', precision)\n",
    "print('recall    =', recall)\n",
    "print('f1_score  =', custom_f1_score(labels, camembert_preds, nb_labels, average= None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2184c7f-7fa9-4e29-8bc6-94f34a0930e2",
   "metadata": {},
   "source": [
    "## Collecting results among sample of trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03a5a330-4d54-4415-9470-58df81abf884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1_scores=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d835eb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "label_names = ['O', 'Cp', 'FC', 'D', 'SO2', 'AGPR', 'G', 'CI']\n",
    "nb_labels = len(label_names)\n",
    "\n",
    "prediction_trainer = pl.Trainer(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a23fa27-8445-4adb-9b1e-cd3d89709da0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_xval: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_xval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_xval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_xval were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../camembert-bio-model were not used when initializing CamembertForTokenClassification_xval: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification_xval from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification_xval from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification_xval were not initialized from the model checkpoint at ../camembert-bio-model and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 7/7 [00:00<00:00, 11.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#path = '../checkpoints/xval2_camembert_tok'\n",
    "path = '../checkpoints/xval2_camembert_tok_lesa'\n",
    "\n",
    "for filename in os.listdir(path):    \n",
    "    checkpoint_path = os.path.join(path, filename)\n",
    "    camembert_tok = LightningModel.load_from_checkpoint(checkpoint_path= checkpoint_path)\n",
    "    camembert_preds = prediction_trainer.predict(camembert_tok, dataloaders=test_dataloader)\n",
    "    camembert_preds = [batch_preds.view(-1) for batch_preds in camembert_preds]\n",
    "    camembert_preds = torch.cat(camembert_preds, -1)\n",
    "    \n",
    "    # collecting the labels\n",
    "    labels = []\n",
    "    it = iter(test_dataloader)\n",
    "    exit = False\n",
    "    while not exit:\n",
    "        try:\n",
    "            # Samples the batch\n",
    "            tokenized_batch = next(it)\n",
    "            label = tokenized_batch['labels']\n",
    "            labels.append(label.view(-1))\n",
    "        except StopIteration:\n",
    "            exit = True\n",
    "    labels = torch.cat(labels, -1)\n",
    "    \n",
    "    f1_score = custom_f1_score(labels, camembert_preds, nb_labels, average= None)\n",
    "    f1_scores.append(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1315438d-c29e-409e-a06b-7ecf514684c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.98894956, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]),\n",
       " array([0.9690077 , 0.        , 0.        , 0.        , 0.19277106,\n",
       "        0.        , 0.        , 0.        ])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab4b1763-1e73-4c4a-baf1-c13075442aad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99147411, 0.        , 0.33939144, 0.46152437, 0.48000128,\n",
       "       0.62043578, 0.3447996 , 0.20099754])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normal xval2\n",
    "average, std = stats(f1_scores)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f7850d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00149897, 0.        , 0.09256756, 0.13617706, 0.15368698,\n",
       "       0.11845154, 0.14788489, 0.14761816])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7143dd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98866672, 0.        , 0.29476533, 0.481562  , 0.40216464,\n",
       "       0.3238592 , 0.        , 0.12054146])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lesa xval2\n",
    "average, std = stats(f1_scores)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7cc4e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00237697, 0.        , 0.11647507, 0.07861173, 0.09929846,\n",
       "       0.06139502, 0.        , 0.10661928])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58642a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal dataset\n",
    "average, std = stats(f1_scores)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df0d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([0.99908665, 0.90285707, 0.96295646, 0.93019133, 0.9433118 ,\n",
    "       0.97099422, 0.87677542, 0.83167779])/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91e03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#blind dataset\n",
    "average, std = stats(f1_scores)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a517d39d-2afd-4ade-8dd3-f4424aed153c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum([0.99029107, 0.58714279, 0.50960482, 0.46767567, 0.69331114,\n",
    "       0.91340833, 0.49791413, 0.65334018])/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2744cb7-04f5-4646-b06b-ee6db7c4b9cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e9490-f142-40bd-91b1-16e45715bd9b",
   "metadata": {},
   "source": [
    "## Some illustrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd6bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_extended_attention_mask(attention_mask):\n",
    "    # Provided a padding mask of dimensions [batch_size, seq_length]\n",
    "    # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "    extended_attention_mask = attention_mask[:, None, None, :]\n",
    "    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "    # masked positions, this operation will create a tensor which is 0.0 for\n",
    "    # positions we want to attend and the dtype's smallest value for masked positions.\n",
    "    # Since we are adding it to the raw scores before the softmax, this is\n",
    "    # effectively the same as removing these entirely.\n",
    "    extended_attention_mask = extended_attention_mask.to(dtype=torch.float)  # fp16 compatibility\n",
    "    extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(torch.float).min\n",
    "    return extended_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4f196-f377-4905-b794-630ec62a684e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#selection of the batch\n",
    "it = iter(test_dataloader)\n",
    "exit = False\n",
    "while not exit:\n",
    "    try:\n",
    "        # Samples the batch\n",
    "        tokenized_batch = next(it)\n",
    "        sentences = decode_properly(tokenized_batch[\"input_ids\"], tokenizer)\n",
    "        for sentence in sentences:\n",
    "            if sentence.startswith('Née à terme, Grossess et accouchement sans complication PN'):\n",
    "                exit = True\n",
    "    except StopIteration:\n",
    "        exit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57858f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "camembert_tok_classifier.eval()\n",
    "with torch.no_grad():\n",
    "    model_output = camembert_tok_classifier.model(\n",
    "            input_ids = tokenized_batch[\"input_ids\"].cuda(),\n",
    "            attention_mask = tokenized_batch[\"attention_mask\"].cuda(),\n",
    "            output_hidden_states=True\n",
    "    )\n",
    "sentences = decode_properly(tokenized_batch[\"input_ids\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aea83f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_padding_mask = tokenized_batch['attention_mask'].type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130c0ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_padding_mask = get_extended_attention_mask(key_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae6cb4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_id = 11\n",
    "sentence_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e86f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences[sentence_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6263c57f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entry_layer = model_output[\"hidden_states\"][layer_id] #l'entrée de la layer layer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4567e24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attn_output, attn_output_weights = camembert_tok_classifier.model.roberta.encoder.layer[layer_id].attention.self(entry_layer, \n",
    "                                    attention_mask= key_padding_mask.cuda(), \n",
    "                                    output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a27f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "head_id = 4\n",
    "attn_output_head_weights = attn_output_weights[:, head_id, :, :]#attn_output_weights.mean(1)\n",
    "attn_output_head_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fdef5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attn_output_weights_visual = attn_output_head_weights[sentence_id].detach().cpu().numpy()[:15, :15]\n",
    "token_names = tokenizer.convert_ids_to_tokens(tokenized_batch[\"input_ids\"][sentence_id][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce4b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "        attn_output_weights_visual,\n",
    "        annot = attn_output_weights_visual, \n",
    "        cbar=False,\n",
    "        #fmt=\"d\",\n",
    "        xticklabels=token_names,\n",
    "        yticklabels=token_names,\n",
    "        cmap=\"viridis\"\n",
    ")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bd8865-80f1-48f6-95a4-d2a4f521b048",
   "metadata": {},
   "source": [
    "## Completion task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf63cc5-cb93-4f67-b89c-579d06f0ffb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "camembert = CamembertForMaskedLM.from_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9f68a8-24ab-4fa0-9e28-34ffabbf9a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_output = tokenizer(\n",
    "    [\"Patient en détresse respiratoire, gradient VG-VD ad <mask> mmgh.\"],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc8c99-b732-4cc1-b021-738fb62dc207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_output = camembert(**tokenizer_output, output_hidden_states=True)\n",
    "    model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d6626f-d84a-4acd-9798-ed105a4e1c16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_probas_from_logits(logits):\n",
    "    return logits.softmax(-1)\n",
    "\n",
    "\n",
    "def visualize_mlm_predictions(tokenizer_output, model_output, tokenizer, nb_candidates=10):\n",
    "    # Decode the tokenized sentences and clean-up the special tokens\n",
    "    decoded_tokenized_sents = [sent.replace('<pad>', '').replace('<mask>', ' <mask>') for sent in tokenizer.batch_decode(tokenizer_output.input_ids)]\n",
    "\n",
    "    # Retrieve the probas at the masked positions\n",
    "    masked_tokens_mask = tokenizer_output.input_ids == tokenizer.mask_token_id\n",
    "    batch_mask_probas = get_probas_from_logits(model_output.logits[masked_tokens_mask])\n",
    "\n",
    "    for sentence, mask_probas in zip(decoded_tokenized_sents, batch_mask_probas):\n",
    "        # Get top probas and plot them\n",
    "        top_probas, top_token_ids = mask_probas.topk(nb_candidates, -1)\n",
    "        top_tokens = tokenizer.convert_ids_to_tokens(top_token_ids)\n",
    "        bar_chart = px.bar({\"tokens\": top_tokens[::-1], \"probas\": list(top_probas)[::-1]},\n",
    "                        x=\"probas\", y=\"tokens\", orientation='h', title=sentence, width=800)\n",
    "        bar_chart.show(config={'staticPlot': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8fddb8-3f2f-4d74-ac98-246a64b86d31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_mlm_predictions(tokenizer_output, model_output, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5789c166-ba92-49db-97dc-8dafcba8fbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xval2_venv",
   "language": "python",
   "name": "xval2_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
