{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45684506-7e92-4618-9272-0c9c1fce3e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lomboa00/cnnt/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae011a-764a-4514-b616-05253b3cdc11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Some functions for data processing that will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b98d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('camembert-bio-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983fc8c4-3baa-4309-8ccf-3897e33cd2d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def categorical(prob, n_samples):\n",
    "    \"\"\"\n",
    "    sample a categorical distribution from a vect of probabilities\n",
    "    \"\"\"\n",
    "    prob = prob.unsqueeze(0).repeat(n_samples, 1)\n",
    "    cum_prob = torch.cumsum(prob, dim=-1)\n",
    "    r = torch.rand(n_samples, 1)\n",
    "    # argmax finds the index of the first True value in the last axis.\n",
    "    samples = torch.argmax((cum_prob > r).int(), dim=-1)\n",
    "    return samples.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28cf12-8f60-4ddf-8ec2-b804ec0f7caf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    if (len(data) > 0) and (data[-1] == ' '):\n",
    "        return data[:-1]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b295b396-d3e3-47d1-a3c8-20e9e0206c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(word):\n",
    "    #print(word, len(word))\n",
    "    if len(word) == 0:\n",
    "        return []\n",
    "    if len(word) == 1:\n",
    "        return [word]\n",
    "    \n",
    "    if word[0] in [':', ',', '.']:\n",
    "        return [word[0]] + remove_punctuation(word[1:])\n",
    "    \n",
    "    idx = 0\n",
    "    while (idx < len(word)) and (not word[idx] in ['!', '(', ')', ';']) :\n",
    "        idx += 1\n",
    "    \n",
    "    if idx == 0:\n",
    "        return [word[0]] + remove_punctuation(word[1:])\n",
    "    if idx == len(word):\n",
    "        if word[-1] in ['.', ',', ':']:\n",
    "            return [word[:-1], word[-1]]\n",
    "        else:\n",
    "            return [word]\n",
    "    \n",
    "    return [word[:idx], word[idx]] + remove_punctuation(word[idx+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0fb00a-dd89-4cd1-a9c4-e3fdfdff52c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxiliary function for the custom tokenizer\n",
    "def aux(word):\n",
    "    cap_symbols = ['T', 'P', 'F', 'B', 'A', 'C', 'D', 'U'] #['T', 'P', 'G', 'L', 'M', 'C', 'D', 'U'] for v1\n",
    "    small_symbols = ['d', 'c', 'a', 'f', 'b', 'u', 'p', 'n'] #['d', 'c', 'm', 'f', 'l', 'u', 'p', 'n'] for v1\n",
    "    \n",
    "    result = ''\n",
    "    # find if there are decimals\n",
    "    has_decimals = False\n",
    "    if ('.' in word): #priority to '.', cause no ambiguity\n",
    "        comma_idx = word.index('.')\n",
    "        has_decimals = True\n",
    "        if (',' in word):\n",
    "            print('error ! dot and comma in number')\n",
    "            \n",
    "    elif (',' in word):\n",
    "        comma_idx = word.index(',')\n",
    "        has_decimals = True\n",
    "    \n",
    "    n=len(word)-1\n",
    "    \n",
    "    idx = n\n",
    "    if has_decimals: #tokenizes the decimal part\n",
    "        while idx > comma_idx:\n",
    "            result = word[idx] + small_symbols[(idx-comma_idx-1)] + result\n",
    "            idx -= 1\n",
    "        n = comma_idx - 1\n",
    "    \n",
    "    if n > 7:\n",
    "        return \"\"\n",
    "    idx = 0\n",
    "    while idx <= n: #tokenizes the main part\n",
    "        result = word[n-idx] + cap_symbols[-(idx+1)] + result\n",
    "        idx += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23a5cf-3b9a-4b80-b7bc-93aeff862a8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Various number formatting and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07631bc6-9b68-4eb4-a6ed-5181b3bd9bf7",
   "metadata": {},
   "source": [
    "## 2.1. Here we define our custom tokenizer designed to handle numbers by breaking them down.\n",
    "See the bank of tests below for a more detailed understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec127db-50b6-4e9f-8877-f3dce9dd4c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_tokenizer(word):  \n",
    "    #isolate numbers\n",
    "    #number surounded  \n",
    "    result = re.sub(r\"([^\\d])(\\d+[\\.,]\\d+)([^\\d])\", r\"\\1 \\2 \\3\", word)\n",
    "    result = re.sub(r\"([^\\d\\.,\\s])(\\d+)([^\\d\\.,\\s])\", r\"\\1 \\2 \\3\", result)\n",
    "    \n",
    "    result = re.sub(r\"([^\\d])(\\d+[\\.,]\\d+)([^\\d])\", r\"\\1 \\2 \\3\", result)\n",
    "    result = re.sub(r\"([^\\d\\.,\\s])(\\d+)([^\\d\\.,\\s])\", r\"\\1 \\2 \\3\", result)\n",
    "    \n",
    "    #number at the beginning\n",
    "    result = re.sub(r\"(^|\\s)(\\d+[\\.,]\\d+)([^\\d\\s])\", r\"\\1\\2 \\3\", result)\n",
    "    result = re.sub(r\"(^|\\s)(\\d+)([^\\d\\.,\\s])\", r\"\\1\\2 \\3\", result)\n",
    "\n",
    "    #number at the end\n",
    "    result = re.sub(r\"([^\\d\\s])(\\d+[\\.,]\\d+)(\\s|$)\", r\"\\1 \\2\\3\", result)\n",
    "    result = re.sub(r\"([^\\d\\.,\\s])(\\d+)(\\s|$)\", r\"\\1 \\2\\3\", result)\n",
    "    \n",
    "    #tokenize isolated numbers\n",
    "    result = re.sub(r\"(^|\\s)(\\d+[\\.,]\\d+)(\\s|$)\", lambda x: x.group(1)+aux(x.group(2))+x.group(3), result)\n",
    "    result = re.sub(r\"(^|\\s)(\\d+)(\\s|$)\", lambda x: x.group(1)+aux(x.group(2))+x.group(3), result)\n",
    "    \n",
    "    #cleaning (ex -/ or /-)\n",
    "    result = re.sub(r\"\\s\", r\"-\", result)\n",
    "    result = re.sub(r\"-([^A-Za-z0-9><])\", r\"\\1\", result)\n",
    "    result = re.sub(r\"([^A-Za-z0-9><])-\", r\"\\1\", result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8181fb-ec70-47e1-bfa4-3cf883955a7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.1.1. Bank of tests for the custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcb5e7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G-2U-P-2U'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"G2P2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4541ecc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G-4U-P-2U-A-1U'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"G4P2A1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ddef3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2D2U-q-1D1U5d'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"22q11.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8642fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cm-2U/m-3U'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"cm2/m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2669ec6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spo-2U'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"Spo2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca71b725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1C2D3U5d6c'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tokenizer(\"123.56\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260ab26d-a6a2-4db8-88b6-fb5cd311325b",
   "metadata": {},
   "source": [
    "## 2.2. Here we implement a function to handle artefacts and noise in the text.\n",
    "See the bank of tests examples for a better understanding of what this function does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc191be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate(word):\n",
    "    # deals with apostrophes at the end of numbers\n",
    "    result = re.sub(r\"(\\d)'$\", r\"\\1\", word)\n",
    "    #convert scientific notation to arabic notation\n",
    "    result = re.sub(r\"(^|\\s)(\\d+e\\d+|\\d+E\\d+|\\d+e-\\d+|\\d+E-\\d+)\", lambda x: x.group(1)+str(float(x.group(2))), result)\n",
    "    #creates a space separation for percentages\n",
    "    result = re.sub(r\"(\\d)%\", r\"\\1 %\", result)\n",
    "    #separates equalites (FR=50)\n",
    "    result = re.sub(r\"=([^\\s])\", r\" = \\1\", result)\n",
    "    # deals with certain codes 2q11.5q12-->2q11.5 - 2q12\n",
    "    result = re.sub(r\"(^|\\s)(\\d+)(q|p)(\\d+[\\.,]\\d+|\\d+)-?(p|q|/)(\\d+[\\.,]\\d+|\\d+)\", r\"\\1\\2\\3\\4 - \\2\\3\\6\", result)\n",
    "    result = re.sub(r\"(^|\\s)(\\d+)(q|p)(\\d+[\\.,]\\d+|\\d+)-?(p|q|/)(\\d+[\\.,]\\d+|\\d+)\", r\"\\1\\2\\3\\4 - \\2\\3\\6\", result)\n",
    "    result = re.sub(r\"(^|\\s)(\\d+)(q|p)(\\d+[\\.,]\\d+|\\d+)-?(p|q|/)(\\d+[\\.,]\\d+|\\d+)\", r\"\\1\\2\\3\\4 - \\2\\3\\6\", result)\n",
    "    result = re.sub(r\"(^|\\s)(\\d+)\\.(q|p)\\.(\\d+[\\.,]\\d+|\\d+)\", r\"\\1\\2\\3\\4\", result)\n",
    "    \n",
    "    #separates range of numbers (except apgar) and operations (32+6)\n",
    "    result = re.sub(r\"(\\d|%)(-+|\\++|/+|:+|x+|X+|\\|+|>+|<+|\\*|~)(\\d)\", r\"\\1 \\2 \\3\", result)\n",
    "    #runs again in case there is apgar or a date\n",
    "    result = re.sub(r\"(\\d|%)(-+|\\++|/+|:+|x+|X+|\\|+|>+|<+|\\*|~)(\\d)\", r\"\\1 \\2 \\3\", result)\n",
    "\n",
    "    # deals with certain numbers representation 2kg48-->2.48kg\n",
    "    result = re.sub(r\"(^|\\s)(\\d+)(kg|KG|Kg)(\\d+)\", r\"\\1\\2.\\4\\3\", result)\n",
    "    # deals with time representation 2h48-->2 h 48\n",
    "    result = re.sub(r\"(^|\\s)(\\d+)(h|H)(\\d+)\", r\"\\1\\2 \\3 \\4\", result)\n",
    "    \n",
    "    #specifically targets apgar (8.8.8) and dates\n",
    "    result = re.sub(r\"^(\\d+)(\\.|,)(\\d+)(\\.|,)(\\d+)\", r\"\\1 - \\3 - \\5\", result)\n",
    "    #deals with lists\n",
    "    result = re.sub(r\"(\\s|^)(\\d+[\\.)])([A-Za-zÀ-ÿ]{2,})(\\s|$)\", r\"\\1item) \\3\\4\", result)\n",
    "    \n",
    "    # remove noise at the end of numbers (12-->)\n",
    "    result = re.sub(r\"(^|\\s)(\\d+[\\.,]\\d+)\\.([^A-Za-z0-9\\s]+)\", r\"\\1\\2 \\3\", result)\n",
    "    result = re.sub(r\"(^|\\s)(\\d+[\\.,]\\d+)([^A-Za-z0-9\\s\\.]+)\", r\"\\1\\2 \\3\", result)\n",
    "    if not re.search(r\"(^|\\s)(\\d+[\\.,]\\d+)\", result): #when the format d,d is not detected\n",
    "        result = re.sub(r\"(^|\\s)(\\d+)\\.([^A-Za-z0-9\\s]+)\", r\"\\1\\2 \\3\", result)\n",
    "        result = re.sub(r\"(^|\\s)(\\d+)([^A-Za-z0-9\\s\\.]+)\", r\"\\1\\2 \\3\", result)\n",
    "        \n",
    "    result = re.sub(r\"(\\d)(:|/)([A-Za-z])\", r\"\\1 \\2 \\3\", result)\n",
    "    result = re.sub(r\"(\\d),([A-Za-z])\", r\"\\1 , \\2\", result)\n",
    "    result = re.sub(r\"(\\d)\\.([A-Za-pr-z])\", r\"\\1 \\2\", result)\n",
    "    # remove noise at the beginning of numbers (-->12)\n",
    "    result = re.sub(r\"(\\s|^)([^A-Za-z0-9\\s]+)(\\d+[\\.,]\\d+|\\d+)\", r\"\\1\\2 \\3\", result)\n",
    "    result = re.sub(r\"([A-Za-z]+),(\\d)\", r\"\\1, \\2\", result)\n",
    "    result = re.sub(r\"([A-Za-pr-z]+)\\.(\\d)\", r\"\\1 \\2\", result)\n",
    "    result = re.sub(r\"([A-Za-pr-z]+)(\\d+[\\.,]\\d+)\", r\"\\1 \\2\", result)\n",
    "    result = re.sub(r\"(^|\\s)(TA|TVC|FC|Fc|fc|min|q|Q)(\\d)\", r\"\\1\\2 \\3\", result) \n",
    "    \n",
    "    #separates numbers and units\n",
    "    result = re.sub(r\"(\\s|^)(\\d+[\\.,]\\d+|\\d+)([A-Za-pr-z])\", r\"\\1\\2 \\3\", result)\n",
    "    # deals with operations within texts (x12 or SA+2jr)\n",
    "    result = re.sub(r\"(x|\\+|X|:|<|>|/|-|«)(\\d)\", r\" \\1 \\2\", result)\n",
    "    result = re.sub(r\"([A-Za-z0-9])->\", r\"\\1 ->\", result)\n",
    "    result = re.sub(r\"->([A-Za-z0-9])\", r\"-> \\1\", result)\n",
    "    \n",
    "    # deals with LetterDigit codes (J2, ccq3h)\n",
    "    result = re.sub(r\"(^|\\s)([JGSjgs])(\\d+)(\\s|$)\", r\" \\1\\2 \\3\\4\", result)\n",
    "    result = re.sub(r\"ccq(\\d+)\", r\" cc q \\1\", result)\n",
    "    # remove noise at the beginning of words (S02->SO2)\n",
    "    result = re.sub(r\"(^|\\s)(-|\\.|,|\\+)([A-Za-z])\", r\"\\1\\2 \\3\", result)\n",
    "    # remove noise at the beginning of words (-SO2)\n",
    "    result = re.sub(r\"([^0-9])02(\\s|$)\", r\"\\1O2\\2\", result)\n",
    "    # remove noise at the end of words (SO2-)\n",
    "    result = re.sub(r\"([^\\s])-(\\s|$)\", r\"\\1 - \\2\", result)\n",
    "\n",
    "    return result\n",
    "\n",
    "#we run it twice to ensure all the issues are solved\n",
    "def minor_edit(word):\n",
    "    edited_word = intermediate(word).split()\n",
    "    result = []\n",
    "    for token in edited_word:\n",
    "        result.append(intermediate(token))\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff682b3c-a962-4f81-b277-d28a479ec4b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2.1. Bank of tests for the artefacts remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc83eadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100 - 115 %'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"100-115%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19f1c5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9 - 8 - 9'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"9-8-9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdb5cba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20 % > 30 %'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"20%>30%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48edff48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8 h 30'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"8h30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63b25cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'22 h 50 min'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"22h50min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81582f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SpO2'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"SpO2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b9c4a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23,2 cm'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"23,2cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6b8fb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 m'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"2m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f389c394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'14 / 08'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"14/08\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce71d938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.23 : 23,1 || 12'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"1.23:23,1||12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e58883a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 m - 3 m'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"2m - 3m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd917d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'179'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"179'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "875f4d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'32 + 6'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"32+6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87a45cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'14.5 , svo2'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"14.5,svo2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "756db9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2q11'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit('2q11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbbe23ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2q11'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"2.q.11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cda16a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'22q11 / Syndrome'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"22.q.11/Syndrome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bdc3c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'19q13 - 19q42 - 19q13 - 19q43'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"19q13/42q13/43\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b2ad7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1q21.1 - 1q21.2'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit('1q21.1q21.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08452521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2q32.1 - 2q34'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit('2q32.1-q34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fed25a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50 cc q 3 h'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"50ccq3h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "202b9940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q 3 - 4 h q 3 h et q 2 sem'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"q3-4h q3h et q2sem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57de56b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q 20 min -> q 1 h -> q 2 h'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"q20min->q1h->q2h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5468fb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12 --> 14'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"12-->14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8d9a6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7,19 / 6 -->'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"7,19/6-->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f29c74d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- 9 - 9'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"-9-9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9ca3d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.67 / 2,3 : 7 / min / 1.23 mm2'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"12.67/2,3:7/min/1.23mm2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b33bfc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8 x 3,5 x 87 cm'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"2.8x3,5x87cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20cbc43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.48 kg'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"2kg48\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "116e4f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 - 2 - 9'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"2.2.9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c9266fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 /kg'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"2/kg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3690c880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 :G'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"2:G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc86f52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'< 2 s'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"<2s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9b3ef1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'item) encephalogramme'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"1.encephalogramme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "604c5f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'369 mm2 / m2'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"369mm2/m2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73e57112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11 h 00 - 05 h 00 -->'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit('11h00-05h00-->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "01553474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5000000.0'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"5e6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "639ac900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- FR = 50 = FC 130'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"-FR=50 =FC130\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9dcc3505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x 20 / sem'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"x20/sem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8ebe3598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'J 1'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"J1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e5ac3eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N < 35 mm/m2'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minor_edit(\"N<35mm/m2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc10b052-ee20-473d-92c1-f42a85f66f95",
   "metadata": {},
   "source": [
    "## 2.3. Here we define two functions designed to mask numbers.\n",
    "\n",
    "`number_masking` is used for preprocessing in the\n",
    "[first paper](https://arxiv.org/abs/2404.10171)\n",
    "\n",
    "`number_blinding`is used for preprocessing in the [second paper](https://arxiv.org/html/2405.18448v2)\n",
    "\n",
    "See the bank of test examples for a better understanding of how these function do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad421185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the blinded word with a number corresponding to its coefficient (see Xval paper)\n",
    "def number_blinding(word):\n",
    "    #single number\n",
    "    if re.search(r\"^(\\d+[,\\.]\\d+|\\d+\\.?)$\", word):\n",
    "        #1 is very often used as a determinant than a number\n",
    "        if word == \"1\":\n",
    "            return word, 1\n",
    "        return \"NUM\", float(re.sub(r\",\", r\".\", word))\n",
    "    if re.search(r\"^(\\d+[,\\.]\\d+|\\d+)\\.$\", word):\n",
    "        return \"NUM\", float(re.sub(r\",\", r\".\", word[:-1]))\n",
    "\n",
    "    #if re.search(r\"\\d\", word) and not re.search(r\"O2$\", word):\n",
    "    #    print(word, custom_tokenizer(word))\n",
    "    return word, 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "211eec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the blinded word\n",
    "def number_masking(word):\n",
    "    #single number\n",
    "    if re.search(r\"^(\\d+[,\\.]\\d+|\\d+\\.?)$\", word):\n",
    "        #1 is very often used as a determinant than a number\n",
    "        if word == \"1\":\n",
    "            return word\n",
    "        return \"nombre\"\n",
    "    if re.search(r\"^(\\d+[,\\.]\\d+|\\d+)\\.$\", word):\n",
    "        return \"nombre\"\n",
    "\n",
    "    #if re.search(r\"\\d\", word) and not re.search(r\"O2$\", word):\n",
    "    #    print(word, custom_tokenizer(word))\n",
    "    return word\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71813fe-640e-4d33-8f17-24fd0bdfe244",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.3.1. Bank of test examples for number_masking and number_blinding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d8998a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SpO2', 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_blinding(\"SpO2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14c9dc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SpO2'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_masking(\"SpO2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1416c7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1', 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_blinding(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c7923a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_masking(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "568099c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NUM', 8.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_blinding(\"8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34a11052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nombre'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_masking(\"8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f288f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NUM', 12.15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_blinding(\"12,15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e387a388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nombre'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_masking(\"12,15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9941881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NUM', 12.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_blinding(\"12,0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f951516f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nombre'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_masking(\"12,0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bacee2-5bbe-4d5f-9464-f503bbe87f73",
   "metadata": {},
   "source": [
    "## 2.4. This function performs scientific notation formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3724111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns significand and exponent\n",
    "def scientific_notiation(num):\n",
    "    result = \"{:e}\".format(num)\n",
    "    exponent = re.sub(r\"^[\\d\\.]*e([\\+-]\\d+)$\", r\"\\1\", result)\n",
    "    significand = re.sub(r\"^([\\d\\.]*)e[\\+-]\\d+$\", r\"\\1\", result)\n",
    "    return float(significand), int(exponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "82513767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2, -1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scientific_notiation(0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b9b7d91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.013, 3)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scientific_notiation(2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f74d840-6315-41a2-8cf3-83ca5f2c975b",
   "metadata": {},
   "source": [
    "# 3. Preprocessing the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b1b93-453e-4279-a988-99caddda756b",
   "metadata": {},
   "source": [
    "## 3.1. Opening documents\n",
    "Use the right encoding and some useless characters\n",
    "\n",
    "Before running these lines, create a sadcsip folder and put the two data files `NLP_data/2020.06.03_CHUSJ_Data_PatientID.csv`and `NLP_data/Labelling Le - 0 to 100.csv`in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5c170f98-da51-47e5-b979-493a4a8e2083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"sadcsip/NLP_data/2020.06.03_CHUSJ_Data_PatientID.csv\", 'r', encoding = 'ISO-8859-1') as file: #latin-1 delimiter = '\\t'\n",
    "    csvreader = csv.reader((line.replace('\\0','').replace('\\t-', '').replace('\\t', '').replace(' \\x19', \"'\") for line in file))\n",
    "    notes = []\n",
    "    for row in csvreader:\n",
    "        notes.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3315b200-62e9-413d-b9be-1497c7115552",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sadcsip/NLP_data/Labelling Le - 0 to 100.csv\", 'r', encoding = 'utf-8') as file:\n",
    "    csvreader = csv.reader((line.replace('\\0','') for line in file))\n",
    "    labels = []\n",
    "    for row in csvreader:\n",
    "        row[2] = clean_data(row[2])\n",
    "        labels.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5f9b48-a69f-49ec-ae4c-6239ab8da19e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.2. Cleaning some minor text mistakes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a1888200-42bc-4007-a9e3-7cb4042a3aea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes[20][2] = 'Né à 41 semaines, 1ère grossesse, pas complication C/s élective, Apgar 8-9 Cardiopathie dépistée en anténatal. Amnio: CGH normal Écho le 23.01: FOP 3-4 mm. Hypoplasie VG. Atrésie mitrale. Pas IT. CIV musc. inlet 5,5 mm.VDDI. Atrésie pulmonaire avec plancher valve pulmonaire non perforé bombant sous v. Ao. Valve Ao remaniée sans fuite significative, gradient 9 mmHg. CA large 3,5 mm shuntant G-D non restrictif. Branches pulmonaires tailles normales. Hypoplasie anneau et APP 4,5mm. Pas de flux visualisé VD à APP. distribution coronarienne N. Arc Ao G normal. Rashkind le 25.01 Sous prota 0,025mcg/kg/min depuis la naissance KT le 27.01: échec dilatation v.pulmo. épanchement péricardiaque post-ponction  Vu par génétique: cardiopathie isolée, CGH -, pas autre test nécessaire'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4044bce0-0bb6-4daf-bcde-381d6c356a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes[46][2] = \" DAN de TOF à 23 SA.  Naissance à 40 SA. PN 2kg820 PC 30-32 cm    Tétralogie fallot extrême       -29/07/2011, à J10patch transannulaire sans fermeture de CIV.       -12/09/2011, à 2 mois de vie, correction chirurgicale complète : patch reprise et fermeture de la CIV. Chylothorax en post opératoire.       -07/11/2011, à 4 mois de vie, angioplastie par stents des 2 artères pulmonaires.       -03//02/2012, à 7 mois, nouveau KT pour défaillance cardiaque droite et PVD supra-systémique. On réalise des angioplasties multiples des artères pulmonaires proximales (a/n stent APG et stent APD) et lobaires avec légère amélioration des pressions dans le VD mais qui demeurent isosystémiques. PVD 74 mmHg. G° VD-APP distal 28mmHg.       -Mise en évidence de resténose des APs avec PVD suprasystémique motivant une nouveau KT le 11/06/2012 à 11 mois.          Patient reste avec pressions VD isosystémique; toutefois la plupart du gradient semble provenir des stents qui sont à leur capacité maximale de dilatation                    percutanée secondaire à prolifération intimale.  Présence d'un anévrysme de l'APG (LIG) de novo après dernière dilatation et qui devra être suivi par imagerie.          (PressionS VD 95 AP 83 Pré stent 79 Post stent 45) Dernière écho cardique 09/04/2013 TOF extreme. S/P correction chirurgicale. SPP bilatérale S/P implantation de stents. Post dilatation par ballon des stents des 2 AP. SPP bilatérale S/P implantation de stents. Patch de CIV sans évidence de résiduelle. Pas d`IAo. Légère accélération dans la voie de chasse du VD ( 15mmHg) . IP légère à modérée avec un reflux diastolique uniquement dans l`APD ( pas de reflux dans l`APP et l`APG). Stents a/n des branches pulmonaires visualisés. Branche droite proximale mesurée à 4.2 mm. Branches gauche mesurée 4.2 mm. Gradient non mesuré a/n des branches  IT légère excentrique, gradient VD-OD de 85 à 90 mmHg  Courbure septale de type II + en systole et en diastole. Bonne contractilité VG. .  VD dilatée et hypertrophié. Fonction VD diminuée qualitativement  Stridor Post 3 extubation (07/2011, 09/2011, 11/2011)    Scopie le 11/2011: Oedème de la glotte    Bronchoscopie rigide 03/02/2012 : N    Pas de lésion de la ss-glotte  Hospit X2 déc 2012 Bronchospasme/Laryngo-trachéo-bronchite  Microcéphalie/Retard de croissance (déjà vu en génétique à la naissance)   FISH 22q11 nég \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0a8efd62-42d7-4f6d-b4c3-4e5ecbb7e404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes[174][2] = 'naissance 37 +5 PN 3.2 D TGV connue Rashkind J1, CIA 7.2 mm Poursuite Pg 0.02 Defaillance x 22.02.2013: CPAP, LANOXIN, LASIX. Tchypnée, pas de tirage, pas de lactates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "976b8278-9338-482b-acad-a4134c9b0daa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes[300][2] = \"1) Née à 38 2/7 semaines. PN 2900 g Diagnostic anténatal de cardiopathie complexe avec T4F et CAVC. APGAR 9 - 9 - 9. SpO2 la naissance 75-80% Mise rapidement sous Inderal.   2) Écho post natale: (S,D,S). Situs solitus. Lévocardie. Arc aortique gauche. Concordance AV et VA. Retour veineux pulmonaire normal. Canal AV complet type A de Rastelli shuntant G-D. Valve AV commune balancée avec insuffisance légère. TOF avec sténose pulmonaire mixte infundibulaire et valvulaire avec CIV de 5.25 mm par déviation antérieure du septum conal. Gradient VD-APP pic de 30 mmHg (sténose infundibulaire en lame de sabre à 9 mmHg). VCSG se drainant dans l`oreillette gauche. Petite VCSD drainant dans l`oreillette droite, sans TVI. CIA très large confinant à l`oreillette unique. petit SIA résiduel. Petit canal artériel perméable tortueux à shunt G-D avec gradient max à 17 mmHg. VCI visualisée, sans interruption.   3) Dilatation de la valve pulmonaire le 03/05/2013 Contexte de désaturation avec besoin en oxygène et majoration du gradient VD AP de 30 à 73mmHg (CA petit non contributif). Évolution post KT amélioration progressive des besoins en O2 jusqu'à sevrage Congés avec Sat entre 75-85% en AA  4) Sept 2013 et janvier 2014 : Suspicion de bactériémie à Staphylocoques epidermidis et S saprophyticus en contexte fébrile. Traitement par Cefotaxime puis cloxacilline. hemocultures de contrôle negatives avec  bilan infectieux normal évoquant une contamination, amélioration avec hydratation et évolution.  5) Mars 2014: IVRS avec hemoc négative, sans surinfection pulmonaire d'évolution favorable après 48h de cefotaxime.  *** Bilan pré-op: - ECG (11/02/2014) Rythme atrial ectopique, hypertrophie bi ventriculaire (T positive en V1 et S profonde en V1 V2) QRS fins. Axe QRS à  \\x13 50. PR normal, QT  normal - Rx: Cardiomégalie avec débord de l'arc inférieur droit, vascularisation pulmonaire normale, silhouette thymique, pas d'anomalies pleuroparenchymateuses - Echographie cardiaque (11/02/2014): Tétralogie de Fallot avec CAVC  s/p dilatation de la valve pulmonaire par KT interventionnel. CIV large de 9.4 mm shuntant bi-directionnel, mais prédominance D-G. Sténose valvulaire pulmonaire, infundibulaire avec hypoplasie de l`APP, gradient VD-AP total de 51 mmHg.  Branches pulmonaires proximales visualisées en vue sous-costale et semblent de bon calibre. Insuffisance pulmonaire discrète. Canal artériel non visualisé. Fonction cardiaque qualitativement normale. VCSG avec hémi-azygos drainant dans OG non recherchée ce jour. Oreillette unique. Fuite légère sur VAV commune. - Scanner cardiaque (18/02/2014) Fallot avec canal AV complet et situs ambigu (asplénie et deux poumons de configuration gauche, deux veines caves supérieures). Hypertrophie ventriculaire droite en lien avec une probable obstruction pulmonaire mal visible. CIV de l'inlet et de l'outlet. Oreillette unique. Absence d'anneau vasculaire. Hypoplasie pulmonaire avec une artère pulmonaire principale mesurée à 7,4 mm, une branche droite à 8,5 mm et une branche gauche à 8,1 mm.   Soit NAKATA calculé 283.35\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6e07de89-ea56-417b-8cbb-c27545fe13c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes[122][2] = \"Naissance terme, APGAR 7-9-10 PN 2470 Tétralogie de Fallot découverte à la visite de routine de 3 semaines de vie (prise de poids difficile, SaO2 85-90% aa). Mise initialement sous prostaglandines et transférée à Sainte-Justine. Dx tétralogie de Fallot extrême avec arc aortique D.  KT cardiaque 2013-04-03 : Débit cardiaque 4,41L/min/m2. Tétralogie de Fallot extrême . Arc aortique D. Artère sous claviève G abérente. Pas de CA. Implantation et trajet des coronaires N. Atrésie pulmonaire confirmée avec nombreuses collatérales aorto pulmonaire.  CT scan 2013-03-29 : Arc Ao droit avec sous-clavière gauche aberrante. Pas de diverticule de Kommerell. ToF avec AP de 2.9 mm. AP primitives filiformes mais continuité, nettement hypoplasiques. AP droite 1,6 mm; AP gauche 2.1 mm. Multiples collatérales Aorto-pulm (5). Retour veineux pulmonaire normal au niveau OG.  Ao augmenté de volume avec Ao chevauchante et hypertrophie venticulaire droite significative. 2 petites CIV (mid-musculaire et apex). Signes hyperinflation LM prob secondaire à multiples collatérales qui encerclent la bronche lobaire moyenne. Zones d'atélectasie importante en LSD, LIG, LID et paramédian D au niveau segement supéro-dosrsal du LID. Dernière écho cardiaque 2013-05-15 : Atrésie pulmonaire uniquement valvulaire avec CIV. Valve pulmonaire de 3.48mm sans flux antérograde VD-AP. APD= 2mm. APG = 2.6mm.  CIV membraneuse a extension conale de 10mm avec shunt non restrictif bidrectionnel mais presque exclusivement D-G. Petite CIV mid-musculaire addiotionnelle. Pas de CIV apicale visualisée. IAo minime. Pas d`accélération VG-Ao. Bonne fonction bi-ventriculaire. TAPSE=11.6mm. S`VD=12cm/s. Pas d`épanchement péricardique. Arc aortique droit, sous claviere gauche aberrante. Présence de collatérales aorto-pulmonaires a/n de l`aorte descendante thoracique. Flux rétrograde diastolique a/n de l`aorte abdominale. FOP non visualisée.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1d145b44-f0fb-4443-9e8e-28be386f7645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes[96][2] = \"Atresie tricuspide, hypoplasie VD, L-Malposition des Gros Vaisseaux, hypoplasie aorte transverse sévère S/P Norwood stade I 8j de vie S/P Dilatation pour re-coarctation 2 mois de vie S/P 11/2010:            -Stent pré-monté Genesis 7x12mm (échec de montée de stent Genesis 1910 a/n voie fémorale 6fr)           -Embolisations de 3 collatérales A-P par de multiples coils; embolisation d'un coil a/n de l'artère brachiale gauche.  S/P Glenn 11/2010 S/P 05/2012:           -Dilatation du stent aortique par ballon Powerflex de 12x2 avec dilatation du stent jusqu'à 9.1mm           -ATL de l'artère lobaire supérieure droite           -Par ailleurs: collatérales V-V a/n du TVI à emboliser avant Fontan  Au dossier de la réunion médico-chx de cardiologie du 2013-09-09: Candidat à une dérivation bicavo-pulmonaire. Indice de Nakata de 148mm2/m2 avec scénario du mieux (au KT du 08/2013) Embolisation de l'AMID n'est pas indiquée vu les PAPm Indication de fenestration retenue.  Dernière Écho cardiaque (2014-06-26): Atrésie tricuspide et TGV (S,D,L) s/p DKS, BDG et stents dans Ao transverse. S/P fermeture collaterales veno-veineuses et re dilatation stent crosse aortique. Large CIA chirurgicale  shuntant D-G majoritairement.  Fonction diastolique:  IPM du VG de 0.42 par Doppler tissulaire.  Onde S` du VG à 6.82 cm/s. Bonne contractilité ventriculaire gauche qualitative. Simpson de 65%. Pas d`insuffisance de la valve mitrale. Insuffisance légère à modérée de la néoaorte avec renversement du flot  en proto-diastole dans l`aorte abdominale. Glenn bien perméable et flux laminaire et phasique. Aorte native et DKS bien visualisé en sous-xyphoidienavec flux laminaire. Arc aortique avec stent visualisé, gradient de 13 mmHg sans extension diastolique. Pas d`extension diastolique a/n de l`aorte abdominale. Pas d`épanchement péricardique.  Patient stable à domicile. Sous ASA 80 à la maison.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "47dd4a45-e22f-4e27-acee-f0af50e2883c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes[252][2] = \"11 mois  diagnostic anténatal de de D-TGV. Apgar 8-9-9 Né à 38 4/7 sem, Diagnostic confirmé d-tgv avec large CIV. Déviation postérieur du septum conal en sous pulmonaire avec accélération sous valvulaire et valvulaire pulmonaire.  Pas de atrioseptostomie nécessaire. IRMc : VDDI avec malposition des gros-vaisseaux et sténose pulmonaire. Opéré à 10 jours de vie : Ligature et division du CA, Résection sous néo-aortique, Fermeture par patch de la CIA et de la CIV. Chylothorax post-opératoire Switch Artériel et manoeuvre de Lecompte. Pas de sténose Aortique ou Insuffisance Aortique. Legere accélération sur les branches pulmonaires. Apparition au courant du suivi d'une obstruction sous néo-aortique de forme diaphragmatique et également d'un conus sous aortique.  Suivi dans la clinique neuro-cardio avec physiothérapie et ergothérapie  Echographie le 5 février 2015: Gradient de pic VG/Ao varie de 60 à 90mmHg , moyen varie de 35 à 43mmHg. Insuffisance aortique discrète. Qualitativement, on note de l` HVG concentrique  légère +. Dilatation de la racine aortique ( diam. 18.2mm cote Z à  2.98 ). IRM 17/02/2015: Status post-correction d'une TGV avec Switch artériel et man\\x01Suvre de Lecompte. Les artères pulmonaires droite et gauche sont de bon calibre et en continuité. Toutefois, il apparaît y avoir une fine membrane juste sous la valve aortique, qui apparaît entraîner principalement l'obstruction de la voie de chasse gauche à l'échographie.  \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b72967bc-2304-4d32-b2c7-9f3a06371dc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes[256][2] = \"1) Née à terme (37+4)     AVS sans complications  2) TOF rose dx anténatal avec belles branches pulmonaires     sat > 98% en néo, congé à J5 après introduction propranolol     bilan malfo N (ETF N, ophtalmo N, écho abdo petite ectasie bassinet bilatéral 5mm à G et 3mm à D)     jamais de crises hypoxiques     asymptomatique d'un point de vue cardiorespiratoire     belle croissance  3) Hernies inguinales bilatérales s/p cure chirurgicale le 15/12/2018        surveillance SIP post-op, pas de complications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e9918727-1af3-4bd0-868a-8a8eb3e0e14c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes[294][2] = \"induction a 38+3SA APGAR 899 PN 2.88kg CPAP en période néonatale Rashkind fait  PG en période néonatale cessé le 29/05 puis repris le 02/06 TDM thoracique 29/05/18: Situs solitus avec boucle ventriculaire droite et transposition droite des gros vaisseaux.  Discordance ventriculo-artérielle et concordance auriculo-ventriculaire.  De plus, malalignement de l'aorte avec CIV secondairement à une déviation du septum conal qui entraîne une obstruction sous-pulmonaire. Post-procédure de Rashkind. Pneumomédiastin au moins modéré.  ETF 29/05/18 : normale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "781e5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes[296][2] = \"2 ans 1 mois   T21    Canal AV intermédiaire CIA typeII Arc Aortique G artère sous-clavière abérante HTAP légère Retard pondérale  AP: Mère toxicomane, trouble psychiatrique, famille d'accueil adéquate ex 32 sem  PN 1770g   ETF N Bronchiolite Apnées centrales PSG 03/2014 s/p adénoïdo-amydalectomie  Rx pré-op: lanoxin, Lasix  ETT 06/2014: CIA 5.1 mm shunt G->D, CIV 6mm partiellement colmatée, gradient VG-VD 40 mmHg, courbure septale II diastole, insuffisance valvulaire minime, dilatation cavité droite   KT 2014/03 pour doute HTAP (VD-VG 80 --> 30): HTAP légère  (Qp-Qs??)  Scan cardiaque: Canal AV de type intermédiaire déjà connu avec une artère pulmonaire  principale effectivement augmentée en taille mais sans que je puisse  identifier d'anomalie parenchymateuse pulmonaire outre deux petites zones  de perfusion en mosaïque au lobe supérieur gauche et une zone  d'atélectasie du lobe supérieur droit pour expliquer l'hypertension artérielle  pulmonaire. Arc aortique gauche avec sous-clavière droite aberrante \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5858fc6b-a4fc-440d-9c65-24f6f878c313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes[1410][2] = \"SOP(24/02) par Dr Vobecky Anesthésie: induction au masque avec sevo. SaO2 pré-op 82% AA. IET facile avec tube microcuff 3.0 ballonet, fixé à 11 cm à la narine D. Acide trenexamique bolus 100 mg/kg puis perfusion 10 mg/kg/h Protamine reçue Produits sanguins recus en SOP:            -> Sous pompe: 2 culots et 1/2 plasma            -> Post-Pompe: 1 plaq, 1 cryo, 70 cc culot globulaire, 60 cc plasma  Procédure: Fermeture CIV large avec patch, fermeture CIV musc avec point, switch artériel, transfert des coronaires. section et ligature CA, fermeture CIA CEC 267 min, clampage 2h. 4 fils de pace en place. Drains thoraciques en place. Plèvre D ouverte, plèvre gauche fermée. N'a pas pu avoir de MUF efficace (probleme technique) à la sortie bradycardie sinusale 80-120 traité avec pacing auriculaire à 150.  Sortie sur adre 0,03, milri bolus (50) et perf 0,5, dopa 3, NO 10, Nitro à 2 Écho cardiaque post-op (en SOP): HVG modérée avec diminution de la contractilité légère. Pas d`IM. Pas de gradient VG/Ao. IAo discrète. IT légère avec gradient VD/OD de 30 mmHg. Pas de gradient VD/AP. Large patche de CIV visualisée sans shunt résiduel. Petite CIV musculaire résiduelle avec gradient VG/VD de 30 mmHg. Flux coronarien mal visualisé. Écho de contraste négatif.  À l'arrivée SIP sur milri 0.5, epi 0.03, Nitro 0,5.  Perfusion culot 50 cc/h et plasma 50 cc/h Intubée, ventilée, sat 100%, BEA bilat. NO 10 ppm. Pace AAI 150, TA 50/30 a l'arrivée avec TOG et TVC basses: remplissage culot et PFC acceléré et bolus Ca Absence d'acidose, bonne oxygenation, lactate 1.2, SVO2 45  Sternum ouvert Saignement ad 60ml/hre par les drains thoraciques. A l'arrivé melange de culot et PFC a 100 ml/hre, acide tranexamique a 10mg/kg/hre, avons repeté protamine et tranfusé cryo et plaquettes. Pas hypothermique. iCa 1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "446c0327-adec-47f2-83ed-876a7db2985d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes[5254][2] = \"Transfert de Rouyn. IVRS en début de semaine, hospit X 3 jours, congé à domicile le 15/01 (sat 93-95% AA à ce moment, RSV/influenza nég). Traitement récent pour suspicion de pneumonie d'aspiration, Clavulin terminé en début de semaine. Le 19/01, détérioration respiratoire rapide, épisode de cyanose à la maison, pic fébrile, rapidement emmenée à l'urgence. À l'arrivée, hypotone, grisâtre, sat. 80% AA, tachycarde, tirage généralisé, hépatomégalie. Assistance avec PEEP rapidement débutée, gaz initial 7.28/54. Ceftri X 1. Tentative de Ventolin sans effet.  Écho coeur par Dr. Tak-Tak à Rouyn : HTAP isosystémique (PAP 70 mmHg), cavités droites très dilatées, IT lég-mod, VCI dilatée, dextrocardie, shunt bidi via CIA, cavités gauches normales, fonction systolique préservée mais VG comprimé par VD, impression de flux rétrograde dans veines pulmonaires sup. ?  Loading milrinone par la suite.  Cas géré entre pédiatre Rouyn & USIP CHUSJ depuis environ 19h. En bref, intubation naso-trachéale avec TET 4 ballonnet, induction Kéta + Fenta + Rocu, par anesthésiste. Support H-D avec milri 0.5 + épi 0.03 (pas de souci H-D à l'intubation). Sat. initialement autour de 80-85%, puis augmentée ad 95% sous FiO2 100%. Pas de NO de disponible. Lasix 2/kg. Gaz améliorés ad 7.37/51/28. Lactate 4.5 -> 1.5. FSC avec GB 41 / HB 102 / PLT 315. Dépistage RSV/influenza nég. ALT 290. Na+ 131 -> 135.  Transport Med-Évaq via Val-d'Or. Globalement stable pendant le vol, plusieurs bolus de fenta + rocu X 1. Sat minimale 83, sinon 95-97%, FC 160-170, TAS ad 100. 2 VVP. Dernier gaz per-transport environ 7.35/41.  Éval rapide à l'urgence puis transfert USIP.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "00421ab8-a251-4f7d-9ef6-c4555017ddaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes[6588][2] = \"X2jours prodrome viral avec toux, sans rhinorrhée ni congestion toutefois Pas d'apnée ni cyanose notées ni fièvre objectivée à la maison  X 24h PatientName plus amorphe, diminution de plus de 50% des boires (allaitement maternel exclusif) mère devait réveiller le PatientName pour allaitement diurèse toutefois préservée 4-5 mictions ce jour pas de vomissements, pas de diarrhées, pas de sang dans les selles  Parents consultent dans la soirée à urgence Saint-Jérôme Là-bas T 38,2R Plusieurs épisodes d'apnées 5-10sec nécessitant stimulation, 2 épisodes de brady ad 77 accompagnés de désat ad 80%  Astrup initial en capillaire 7,20/68/26 , contrôle 19h 7,30/50  Bilan septique complet fait avec PL neg et PCR HSV en cours RXP là-bas interprété N, dépistage rapide RSV influenza Neg, urine ? (résultats pas retrouvés au dossier) Début cefo et genta là-bas à doses méningées, acyclovir débutée à l'urgence ici  À l'urgence HSJ  FR 50 Sat 98% AA TAS 70 FC 150-170 3-4 épisodes d'apnées brèves 5-10sec sans désat associée Brady ad 77 X1 avec désat 85% LNHD 7LPM débutée en bas Bolus 20cc/kg X1 re: marbrée, tachycarde 170-180\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2a9c66d0-485a-481a-a520-6a5ec794fa46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes[7692][2] = 'Post-op immédiat amygdalectomie + adenoidectomie + myringotomie bilatérale sous AG Atropine/ propofol/ decadron reçus pour intubation 200cc LR + dexmedetomidine 15mg x 15 min Procédure bien tolérée Réveil spontané en salle de réveil intubé. Sat 100% FC = 90. Extubé en salle de réveil. Vers 10:30, dose morphine 3 mg IV x 1 donné au lieu de 0.3 mg alors que patient était extubé. Patient endormi initialement, mais saturation demeurait normale. Vers 12:15, debut de ronronnement et diminution de la sat à 94%. Boyeau O2 à 28% installé (Patient refusait LN et masque).  Saturation demeurait autour de 90-94%. O2 dans boyau augmenté à 60%.  A mon passage vers 14h00, patient bien éveillé et BEG. Saturation à 96% avec 60% de boyau. Pas de ronflement. Fc= 100 Fr= 20. Pas de tirage. BEA x 2. Bruits de transmission des voies aériennes hautes. Pas de sibilances. pas de crépitants  Observation à la salle de réveil x 2h. Persistance besoin O2. Désaturation ad 80% (objectivé par MD) lorsque patient dort dans les bras de la mère. Augmentation de la saturation avec repositionnement. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e4572246-4ff5-44c7-bbc7-b86e764ed6f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes[126][2] = \"Grossesse sans particularité  GBS- Naissance à 40 1/7 sem- Dystocie des épaules sans complication, Apgar 8-9-9  PN 3,6kg Polypnéique avec plainte expiratoire aux boires depuis la naissance mais bonne prise de poids   Cardiomyopathie dilatée d'origine indéterminée   Hospitalisé USIP du 30/05/2013 au 25/06/2013     ECMO 30/05 au 04/06 avec vent dans VG  IRC Suivi Dr Phan  Écho rénale avril 2014    Rein D 5.5cm G 5.3 cm (peu de croissance depuis la dernière écho)  Évaluation neuro Retard développement suivi Dr Lorti Semblait s'améliorer selon note pédiatre Rouyn (Dr Cardinal) du 14/03/2014 Scan tête le 23-10-2013   Proéminence diffuse des espaces ss-arachnoidiens non spécifique. Normal par ailleurs.  Spasme du sanglot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2e9473d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes[4518][2] = 'Anesthésie: Induction: TET 4.5 grade I, oral Voies:JID 4.8cm, periph x2, radiale droite Saignement: minime  Chirurgie: Fontan non fenestré extracardiaque Nouvelle électrodes ventriculaire + changement boitier Plèvres ouvertes bilatérales  CEC: 91 minutes Sortie: facile, bradycardie 60 sinuale résolue avec dopamine x10minutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7c9aaf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes[9676][2] = \"H 6 ans et demi Transféré de Mont Laurier pour bronchospasme sévère  Arrivé à l'urgence à 1h30 du matin, difficulté respiratoire, dyspnée x 23h la veille  Vu en salle de stab là bas : SaO2 75-80% à l'air libre, obnubilé  Aurait eu IVRS/toux x 4 jours, fièvre (pas mesurée), un peu céphalée, Vox1 hier (liquide), pas diarrhée, pas dlr abdo, pas de myalgies.  Avait quand même énergie (pas au lit affaissé) mais n'est pas aller à l'école   37.7 rectal. Poids estimé 22kg  IET avec l'anesthésiste TET 6.0 À reçu Solumedrol 44mg à 2h00, NS à flush, qtité? Ventolin + Atrovent en nébul, Sulfate de Mg 1.1g (50mg/kg) Ceftriaxone 1.1g (50mg/kg), bolus de D5%(0.45% ou 0.9%) NS 250cc,  Pour intubation 44mg kétamine, versed 2mg, Rocu 30mg, annectine 35mg  Puis mis sous perfusion de kétamine 1.5mg/kg/h + Ventolin IV 0.5-->1.5mcg/kg/min RX poumons : début infiltrat pms G haut du lobe inf vs bas du lobe sup, pas de latéral dispo.  Gaz là bas : 7.21-->7.12 CO2 58-->74 À l'arrivée à l'urgence : pH 7.07/CO2 89\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2e519b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes[6524][2] = \"difficulté au boires depuis 24-48h. Plus tachypneic et détresse au boires. Sudation x 1 Pas de température, pas de toux, pas d'étouffements.  Contact + infectieux chez fraterie.  S'est présenté a Hopital St-Jerome avec sat 85% amélioré ad 100% avec 1L O2, Fc 145, afebrile, RR50 Bolus x 1 NS Cardiomegalie majeure au RXP et transfert HSJ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a1316fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes[2414][2] = \"Suspicion de DiGeorges (Fisch 22q11 négatif par ailleurs - patient refuse d'autres tests génétiques) Déficit immunitaire / thymectomie : 1 dose de IVIG reçu le 07/03 Obesite, Retard des acquisitions mais scolarisation normale Lésions cutanées surtout au niveau des plis (perlèche) avec culture candidose + Présence de kyste pilonidale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4da4a3d2-5660-4402-aae6-fb0587f66804",
   "metadata": {},
   "outputs": [],
   "source": [
    "yes= [2, 4, 22, 16, 52, 31, 23, 43, 67, 64, 32]\n",
    "no = [30, 46, 47, 60, 70, 94, 157, 127, 1, 54, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533c2956-5a6b-4951-ac0f-2f74d663c334",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.3 Fixing little typos related to dates format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "68623f31-f3bf-4cf1-86f5-380e63753eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['860', '3050720', '2016.07.21 : CIV PM, dilatation cavites gauches, stenoses sous pulmonaire, eperon sous aortique, FOP, CIA fermee  Rx lasix 5mg TID prevacid', 'yes']\n",
      "  -Hospit 14.08 au 05.09.2014: Admis dans un contexte de SYNCOPE  avec incontine\n"
     ]
    }
   ],
   "source": [
    "#before processing\n",
    "print(notes[1722])\n",
    "print(notes[62][2][570:650])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9c98d805-566a-47f5-a14f-151bb2e23bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(2, len(notes), 2):\n",
    "    pattern = r\"\\d{2}\\.\\d{2}\\.\\d{2}\"#full day\n",
    "    result = re.finditer(pattern, notes[i][2])\n",
    "    if result:\n",
    "        note = list(notes[i][2])\n",
    "        for m in result:\n",
    "            a, b = m.start(0), m.end(0)\n",
    "            note[a+2]='/'\n",
    "            note[a+5]='/'\n",
    "        notes[i][2] = ''.join(note)\n",
    "    \n",
    "    pattern = r\"\\d{2}\\.\\d{1}\\.\\d{2}\"#full day\n",
    "    result = re.finditer(pattern, notes[i][2])\n",
    "    if result:\n",
    "        note = list(notes[i][2])\n",
    "        for m in result:\n",
    "            a, b = m.start(0), m.end(0)\n",
    "            note[a+2]='/'\n",
    "            note[a+4]='/'\n",
    "        notes[i][2] = ''.join(note)\n",
    "    \n",
    "    pattern = r\"\\d{2}\\.\\d{2}\"#day and month\n",
    "    result = re.finditer(pattern, notes[i][2])\n",
    "    if result:\n",
    "        note = list(notes[i][2])\n",
    "        for m in result:\n",
    "            a, b = m.start(0), m.end(0)\n",
    "            note[a+2]='/'\n",
    "        notes[i][2] = ''.join(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "146f2d36-6f51-478c-af2b-c6245e478bd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['860', '3050720', '2016/07/21 : CIV PM, dilatation cavites gauches, stenoses sous pulmonaire, eperon sous aortique, FOP, CIA fermee  Rx lasix 5mg TID prevacid', 'yes']\n",
      "  -Hospit 14/08 au 05/09/2014: Admis dans un contexte de SYNCOPE  avec incontine\n"
     ]
    }
   ],
   "source": [
    "#after processing\n",
    "print(notes[1722])\n",
    "print(notes[62][2][570:650])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fefb9c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing lists items numbers\n",
    "for i in range(2, len(notes), 2):\n",
    "    matches = re.findall(r\"(\\s\\d{1,2}\\.\\s?[A-Za-ce-z]|^\\d{1,2}\\.\\s[A-Za-z])\", notes[i][2])\n",
    "    if len(matches) > 1 and re.search(r\"1\\.\", matches[0]):\n",
    "        is_enumeration, cursor = True, 1\n",
    "        while is_enumeration and cursor < len(matches):\n",
    "            is_enumeration = (float(matches[cursor][1:3]) == cursor+1)\n",
    "            cursor += 1\n",
    "        if is_enumeration:\n",
    "            notes[i][2] = re.sub(r\"(^|\\s)\\d{1,2}\\.\\s?([A-Za-ce-z])\", r\"\\1item) \\2\", notes[i][2])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb135f1-fa00-4904-af88-9f567c6a5cfa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.4 Listing the doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "91280e1a-d584-47c9-8b87-4c37b0345893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes_without_doublons = [] #collectionne les id des notes sans prendre en compte les doublons\n",
    "list_of_doublons = {} #collectionne les classes d'équivalence de notes\n",
    "deja_vu_notes = [] #collectionne les notes de manière unique\n",
    "for i in range(2, len(notes), 2):\n",
    "    note = notes[i][2]\n",
    "    try:\n",
    "        idx = deja_vu_notes.index(note)\n",
    "        list_of_doublons[idx].append(i)\n",
    "    except ValueError:\n",
    "        notes_without_doublons.append(notes[i])\n",
    "        list_of_doublons[len(deja_vu_notes)] = [i] #l'id des classes d'équivalence est pris comme le rang d'apparition de la classe d'équivalence dans la détection des classes\n",
    "        deja_vu_notes.append(note)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b485975-7ab7-4b88-8af8-0b95f1b21f1b",
   "metadata": {},
   "source": [
    "This line lists all duplicate notes in the dataset. For each duplicated note, the ID of its first occurrence is provided, followed by a list of note IDs corresponding to all other duplicates of that note.\n",
    "\n",
    "(`note_id`, `[note_id of doublon_1, note_id of doublon_2, ...]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c46ed915-91a9-49c9-9ebe-9fc8356fb56b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(103,\n",
       "  [208,\n",
       "   830,\n",
       "   1036,\n",
       "   1968,\n",
       "   2256,\n",
       "   4748,\n",
       "   4784,\n",
       "   4832,\n",
       "   4902,\n",
       "   5070,\n",
       "   5730,\n",
       "   5778,\n",
       "   6218,\n",
       "   6436]),\n",
       " (954, [1970, 2480, 4786, 5452, 5692, 9876]),\n",
       " (793, [1592, 1972, 2502, 10068]),\n",
       " (829, [1664, 1814, 2554, 5258]),\n",
       " (1995, [4668, 5446, 5670, 9102])]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " sorted(list_of_doublons.items(), key= lambda x:len(x[1]), reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4dee304f-d7d0-4f89-a268-18cd5f1e0770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(918, 4796)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(notes_without_doublons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbcb78-7b34-445f-ab39-02931c65d871",
   "metadata": {},
   "source": [
    "## 3.5 Looking among the non-annotated notes to see which ones have a lot of numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dc100b2a-3833-42e6-84c8-a25e11726c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "votes = {k:[] for k in range(159, len(notes_without_doublons))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e63aab65-192b-456d-903a-2900b715046f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#FE , gradient, diamètre and find intersection, Tension Artérielle, Fraction\n",
    "for i in range(159, len(notes_without_doublons)):\n",
    "    try:\n",
    "        ids = notes_without_doublons[i][2].index('FC ')\n",
    "        votes[i].append('FE')\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "71aee944-5d53-481d-845a-b15bdace4d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(159, len(notes_without_doublons)):\n",
    "    try:\n",
    "        ids = notes_without_doublons[i][2].index('APP ')\n",
    "        votes[i].append('APP')\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cf687044-34b1-4ef9-8d97-574368112b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "for i in range(159, len(notes_without_doublons)):\n",
    "    try:\n",
    "        ids = notes_without_doublons[i][2].index('diamètre ')\n",
    "        votes[i].append('diamètre')\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ffca7976-5ac7-464e-ac84-dabbb802476f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "for i in range(159, len(notes_without_doublons)):\n",
    "    try:\n",
    "        ids = notes_without_doublons[i][2].index('Fraction ')\n",
    "        votes[i].append('Fraction')\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "da19b93d-7216-4b2a-8b89-10f7d4f315ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(159, len(notes_without_doublons)):\n",
    "    try:\n",
    "        ids = notes_without_doublons[i][2].index('sat ')\n",
    "        votes[i].append('sat')\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "28065b89-6446-46ab-ad10-f19882c26ed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "votes_counts = {k: len(votes[k]) for k in range(159, len(notes_without_doublons))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2e1ddd4a-085b-4c78-b80d-2810d1362fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "votes_rank = sorted(votes_counts.items(), key= lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e7a88fe1-0667-4343-a52f-7de1adfaabe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_considered_notes = [i for i in range(100)] + [107, 108, 109, 120, 122, 123, 125,127, 130, 131,132, 133, 134, 137, 140, 141, 144, 146, 147, 149, 150, 156, 158]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c785e1bf-2285-4bf3-976a-6d21d9a8fea3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2772, 2),\n",
       " (2773, 2),\n",
       " (2801, 2),\n",
       " (2807, 2),\n",
       " (2836, 2),\n",
       " (2870, 2),\n",
       " (2883, 2),\n",
       " (2918, 2),\n",
       " (2942, 2),\n",
       " (2977, 2),\n",
       " (3022, 2),\n",
       " (3115, 2),\n",
       " (3124, 2),\n",
       " (3128, 2),\n",
       " (3150, 2),\n",
       " (3155, 2),\n",
       " (3235, 2),\n",
       " (3241, 2),\n",
       " (3263, 2),\n",
       " (3272, 2)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes_rank[40:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece6bdf-24c3-479f-aa14-d2dcfc167237",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.5.1. Auto-annotating some of the selected notes, to increase a little bit the dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "970dbebd-d1e1-40c6-9fd3-d6006ebc8abd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7d0cc8af-165c-4d7a-a504-2f09207a9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2.append(['60', '2.6mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['60', '2mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['60', '2.1', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['69', '8,7', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['39', '22', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['66', '3.3', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['76', '2.2-2.4mm', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['76', '5.59mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['107', '20%', 'Saturation en oxygène'])\n",
    "labels2.append(['107', '80%', 'Saturation en oxygène'])\n",
    "labels2.append(['108', '56mmHg', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['108', '14.6', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['109', '8', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['109', '34', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['120', '3-2-3', 'apgar'])\n",
    "labels2.append(['122', '1-4-5', 'apgar'])\n",
    "labels2.append(['123', \"98%-100%\", 'Saturation en oxygène'])\n",
    "labels2.append(['123', '80-100/min', 'Fréquence cardiaque', 'bpm'])\n",
    "#labels2.append(['123', '90-100/50-60', 'Tension artérielle', 'mmHg'])\n",
    "labels2.append(['125', '8-9-9', 'apgar'])\n",
    "labels2.append(['127', '98%', 'Saturation en oxygène'])\n",
    "labels2.append(['130', '9-9-9', 'apgar'])\n",
    "labels2.append(['130', '46mmHg', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['130', '4-5mm', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['130', '8', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['131', '8-9-9', 'apgar'])\n",
    "labels2.append(['132', '7-9-10', 'apgar'])\n",
    "labels2.append(['133', '5-8-8', 'apgar'])\n",
    "labels2.append(['133', '60%', 'Saturation en oxygène'])\n",
    "labels2.append(['133', '60', 'Saturation en oxygène'])\n",
    "labels2.append(['133', '87%', 'Saturation en oxygène'])\n",
    "labels2.append(['134', '90-95%', 'Saturation en oxygène'])\n",
    "labels2.append(['137', '8-8-8', 'apgar'])\n",
    "labels2.append(['140', '6-9-9', 'apgar'])\n",
    "labels2.append(['141', '7mm', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['144', '8.7', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['144', '8.8', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['146', '899', 'apgar'])\n",
    "labels2.append(['147', '40', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['147', '5.1', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['147', '6mm', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['147', '80', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['147', '30', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['149', '75-80%', 'Saturation en oxygène'])\n",
    "labels2.append(['149', '75-85%', 'Saturation en oxygène'])\n",
    "labels2.append(['149', '30', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['149', '30', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['149', '73mmHg', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['149', '51', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['149', '7,4', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['149', '8,5', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['149', '8,1', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['149', '5.25', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['149', '17', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['149', '9.4', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['150', '11', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['150', '16', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['156', '8-8-8', 'apgar'])\n",
    "labels2.append(['158', '9-9-9', 'apgar'])\n",
    "labels2.append(['2153', '106', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2153', '100', 'Saturation en oxygène'])\n",
    "labels2.append(['2153', '30-35', \"Contractibilité\"])\n",
    "labels2.append(['2153', '88', 'Saturation en oxygène'])\n",
    "labels2.append(['2153', '129', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2153', '121', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2153', '91%', 'Saturation en oxygène'])\n",
    "labels2.append(['2153', '96', 'Saturation en oxygène'])\n",
    "labels2.append(['2153', '50', \"Contractibilité\"])\n",
    "labels2.append(['2595', '4.56mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['2595', '4mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['2595', '3.5mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['425', '85-95%', 'Saturation en oxygène'])\n",
    "labels2.append(['425', '55%', 'Saturation en oxygène'])\n",
    "labels2.append(['425', '90-95', 'Saturation en oxygène'])\n",
    "labels2.append(['425', '76', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['425', '3.5', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['425', '3.8', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['582', '30%', 'Saturation en oxygène'])\n",
    "labels2.append(['582', '75-80%', 'Saturation en oxygène'])\n",
    "labels2.append(['582', '4.1mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['582', '4-5mm', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['685', '2.7mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['685', '3.2', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['685', '2.6', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['685', '7.6', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['685', '2', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['685', '3.8', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['685', '3.3', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['685', '3.6', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['685', '3', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['685', '3,7', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['685', '2', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['685', '7mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['685', '5.5', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['685', '6mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['685', '74mmHg', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['685', '11mm', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['704', '82%', 'Saturation en oxygène'])\n",
    "labels2.append(['704', '80-120', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['704', '150', 'Fréquence cardiaque', 'bpm'])\n",
    "#labels2.append(['704', '30', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['704', '100%', 'Saturation en oxygène'])\n",
    "labels2.append(['704', '150', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['721', '75%', 'Saturation en oxygène'])\n",
    "labels2.append(['721', '10mm', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['721', '5mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['981', '160-170', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['981', '85', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['981', '90%', 'Saturation en oxygène'])\n",
    "labels2.append(['981', '6%', 'Saturation en oxygène'])\n",
    "labels2.append(['981', '50', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['981', '80%', 'Saturation en oxygène'])\n",
    "labels2.append(['1201', '30%', 'Saturation en oxygène'])\n",
    "labels2.append(['1201', '75-80%', 'Saturation en oxygène'])\n",
    "labels2.append(['1201', '4.1mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['1201', '4-5mm', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['1384', '95-105', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['1384', '75%', 'Saturation en oxygène'])\n",
    "labels2.append(['1384', '86%', 'Saturation en oxygène'])\n",
    "labels2.append(['1470', '111', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['1470', '30%', 'Saturation en oxygène'])\n",
    "labels2.append(['1578', '180', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['1578', '80%', 'Saturation en oxygène'])\n",
    "labels2.append(['1578', '10mmHG', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['1578', '9mm', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['1578', '67%', 'Saturation en oxygène'])\n",
    "labels2.append(['1578', '27%', 'Saturation en oxygène'])\n",
    "labels2.append(['1725', '170', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['1725', '70-75%', 'Saturation en oxygène'])\n",
    "labels2.append(['1725', '3.3mm', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['1725', '7.8', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['1730', '90-95', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['1730', '120', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['1730', '100%', 'Saturation en oxygène'])\n",
    "labels2.append(['1730', '75%', 'Saturation en oxygène'])\n",
    "labels2.append(['1730', '120', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2207', '100-110', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['2207', '90%', 'Saturation en oxygène'])\n",
    "labels2.append(['2207', '68', 'Saturation en oxygène'])\n",
    "#labels2.append(['2207', '7mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "#labels2.append(['2207', '45mmHg', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['2207', '17mmHg', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['2208', '88-92', 'Saturation en oxygène'])\n",
    "labels2.append(['2208', '20', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['2208', '12-14', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['1930', '130', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['1930', '4.75mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['1930', '4.4mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['1930', '5mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['1930', '3mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['2269', '8mmHg', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['2269', '25', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['2269', '130', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2273', '11mmHG', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['2273', '190-200', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2273', '55', 'Saturation en oxygène'])\n",
    "labels2.append(['2273', '180-185', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2276', '160-170', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2276', '85', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2276', '90%', 'Saturation en oxygène'])\n",
    "labels2.append(['2276', '6%', 'Saturation en oxygène'])\n",
    "labels2.append(['2276', '50', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2276', '80%', 'Saturation en oxygène'])\n",
    "labels2.append(['2475', '3-8-9', 'apgar'])\n",
    "labels2.append(['2475', '97', 'Saturation en oxygène'])\n",
    "labels2.append(['2475', '70-88', 'Saturation en oxygène'])\n",
    "labels2.append(['2475', '160', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2475', '80-83', 'Saturation en oxygène'])\n",
    "labels2.append(['2485', '5.6mm', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['2485', '65%', 'Saturation en oxygène'])\n",
    "labels2.append(['2485', '89%', 'Saturation en oxygène'])\n",
    "labels2.append(['2485', '9.25mm', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['2485', '35%-40%', 'Contractibilité'])\n",
    "labels2.append(['2485', '6.9mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['2485', '5mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['2485', '154', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2514', '200', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2514', '80%', 'Saturation en oxygène'])\n",
    "labels2.append(['2514', '90-96%', 'Saturation en oxygène'])\n",
    "labels2.append(['2514', '25%', \"Contractibilité\"])\n",
    "labels2.append(['2514', '130-140', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2610', '65-72%', 'Saturation en oxygène'])\n",
    "labels2.append(['2610', '170', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2616', '80-90%', 'Saturation en oxygène'])\n",
    "labels2.append(['2616', '3', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['2616', '4', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['2616', '7', 'Taille CIA/CIV', 'mm'])\n",
    "labels2.append(['2616', '12', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['2626', '93-95%', 'Saturation en oxygène'])\n",
    "labels2.append(['2626', '80%', 'Saturation en oxygène'])\n",
    "labels2.append(['2626', '80-85%', 'Saturation en oxygène'])\n",
    "labels2.append(['2626', '95%', 'Saturation en oxygène'])\n",
    "labels2.append(['2626', '95-97%', 'Saturation en oxygène'])\n",
    "labels2.append(['2626', '160-170', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2700', '45', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2700', '40%', 'Saturation en oxygène'])\n",
    "labels2.append(['2700', '50-55', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2700', '50%', 'Saturation en oxygène'])\n",
    "labels2.append(['2700', '45', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2700', '80', 'Fréquence cardiaque', 'bpm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "00e38d57-6828-4828-b2ec-3defb1818f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2.append(['2917', '195', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2917', '100%', 'Saturation en oxygène'])\n",
    "labels2.append(['2917', '80', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2917', '83%', 'Saturation en oxygène'])\n",
    "labels2.append(['2959', '44%', 'Saturation en oxygène'])\n",
    "labels2.append(['2959', '80', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2959', '180', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['2959', '97%', 'Saturation en oxygène'])\n",
    "labels2.append(['3240', '93%', 'Saturation en oxygène'])\n",
    "labels2.append(['3240', '115', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3240', '80%', 'Saturation en oxygène'])\n",
    "labels2.append(['3240', '94%', 'Saturation en oxygène'])\n",
    "labels2.append(['3240', '100-110', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3240', '95%', 'Saturation en oxygène'])\n",
    "labels2.append(['3245', '171', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3245', '96%', 'Saturation en oxygène'])\n",
    "labels2.append(['3245', '130', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3245', '100%', 'Saturation en oxygène'])\n",
    "labels2.append(['3292', '50', 'Saturation en oxygène'])\n",
    "labels2.append(['3292', '50-60', 'Saturation en oxygène'])\n",
    "labels2.append(['3292', '50', 'Saturation en oxygène'])\n",
    "labels2.append(['3292', '60', 'Saturation en oxygène'])\n",
    "labels2.append(['3292', '70%', 'Saturation en oxygène'])\n",
    "labels2.append(['3292', '40', 'Saturation en oxygène'])\n",
    "labels2.append(['3293', '80%', 'Saturation en oxygène'])\n",
    "labels2.append(['3293', '77', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3293', '98%', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3293', '150-170', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3293', '77', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3293', '85%', 'Saturation en oxygène'])\n",
    "labels2.append(['3293', '170-180', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3326', '85%', 'Saturation en oxygène'])\n",
    "labels2.append(['3326', '160', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3326', '150', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3326', '96%', 'Saturation en oxygène'])\n",
    "labels2.append(['3332', '11mmHg', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['3332', '13mmHg', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['3332', '3.7mm', 'Diamètre Artère Pulmonaire', 'mm'])\n",
    "labels2.append(['3332', '21mmHg', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['3369', '92', 'Saturation en oxygène'])\n",
    "labels2.append(['3369', '140', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3369', '94', 'Saturation en oxygène'])\n",
    "labels2.append(['3369', '92', 'Saturation en oxygène'])\n",
    "#labels2.append(['3425', '4mmHg', 'gradient ventriculaire', 'mmHg'])\n",
    "#labels2.append(['3425', '4mmHg', 'gradient ventriculaire', 'mmHg'])\n",
    "#labels2.append(['3425', '2mmHg', 'gradient ventriculaire', 'mmHg'])\n",
    "labels2.append(['3425', '80', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3425', '160/min', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3425', '14%', 'Contractibilité'])\n",
    "labels2.append(['3425', '40%', \"Contractibilité\"])\n",
    "labels2.append(['3425', '99%', 'Saturation en oxygène'])\n",
    "labels2.append(['3425', '14%', 'Contractibilité'])\n",
    "labels2.append(['3463', '98-99%', 'Saturation en oxygène'])\n",
    "labels2.append(['3463', '55%', 'Saturation en oxygène'])\n",
    "labels2.append(['3463', '97%', 'Saturation en oxygène'])\n",
    "labels2.append(['3463', '172', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3463', '97%', 'Saturation en oxygène'])\n",
    "labels2.append(['3463', '173', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3582', '87', 'Saturation en oxygène'])\n",
    "labels2.append(['3582', '91%', 'Saturation en oxygène'])\n",
    "labels2.append(['3582', '124', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3582', '95-96', 'Saturation en oxygène'])\n",
    "labels2.append(['3739', '60', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3739', '78%', 'Saturation en oxygène'])\n",
    "labels2.append(['3739', '100%', 'Saturation en oxygène'])\n",
    "labels2.append(['3739', '114', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3845', '100%', 'Saturation en oxygène'])\n",
    "labels2.append(['3845', '90', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3845', '94%', 'Saturation en oxygène'])\n",
    "labels2.append(['3845', '90-94%', 'Saturation en oxygène'])\n",
    "labels2.append(['3845', '96%', 'Saturation en oxygène'])\n",
    "labels2.append(['3845', '100', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3845', '80%', 'Saturation en oxygène'])\n",
    "labels2.append(['3869', '80%', 'Saturation en oxygène'])\n",
    "labels2.append(['3869', '200', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3869', '91-94%', 'Saturation en oxygène'])\n",
    "labels2.append(['3869', '85%', 'Saturation en oxygène'])\n",
    "labels2.append(['3878', '155', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3878', '145', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3878', '160', 'Fréquence cardiaque', 'bpm'])\n",
    "labels2.append(['3878', '50-55%', \"Contractibilité\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fdfb7d16-b6c8-4915-8958-3902fa682c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_considered_notes += [2153, 2595, 425, 582, 685, 704, 721, 981, 1201, 1384, 1470, 1578, 1725, 1730, 2207, 2208, 1930, 2269, 2273, 2276, 2475, 2485, 2514, 2610, 2616, 2626, 2700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ba47c349-9b2d-4ee4-b765-84d7a3f6b878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_considered_notes += [2917, 2959, 3240, 3245, 3292, 3293, 3326, 3332, 3369, 3425, 3463, 3582, 3739, 3845, 3869, 3878]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac082d-9596-4b02-9bcf-a4095f847ee5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.6. Fixing some minor tokens issues in the annotations provided by the Clinicians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a4c6dc6d-9920-4143-b807-5d072afd7a72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fixing some splitted tokens\n",
    "labels[439][1] = '65%'\n",
    "labels[478][1] = '46%'\n",
    "labels[815][1] = '27%'\n",
    "labels[284][1] = '31.3%'\n",
    "labels[310][1] = '40,7%'\n",
    "labels[480][1] = '29%'\n",
    "labels[100][1] = '4,5mm'\n",
    "labels[175][1] = '5.5mm'\n",
    "labels[176][1] = '6.6mm'\n",
    "labels[381][1] = '2.8mm'\n",
    "labels[382][1] = '2.3mm'\n",
    "labels[486][1] = '4.9mm'\n",
    "labels[487][1] = '5.7mm'\n",
    "labels[655][1] = '18mm'\n",
    "labels[4][1] = '80-85%'\n",
    "labels[19][1] = '87%'\n",
    "labels[165][1] = '85-88%'\n",
    "labels[166][1] = '75%'\n",
    "labels[224][1] = '50-65%'\n",
    "labels[228][1] = '70-75%'\n",
    "labels[394][1] = '25%'\n",
    "labels[401][1] = '96%'\n",
    "labels[529][1] = '80-85%'\n",
    "labels[543][1] = '85-90%'\n",
    "labels[550][1] = '65%'\n",
    "labels[601][1] = '65-85%'\n",
    "labels[603][1] = '75%'\n",
    "labels[738][1] = '92%'\n",
    "labels[13][1] = '8-9-9'\n",
    "labels[24][1] = '8-9-9'\n",
    "labels[328][1] = '9-9-10'\n",
    "labels[340][1] = '8-9'\n",
    "labels[396][1] = '8-9'\n",
    "labels[417][1] = '1-2-3'\n",
    "labels[448][1] = '9-9-9-'\n",
    "labels[545][1] = '7-9-10'\n",
    "labels[563][1] = '8-9-9'\n",
    "labels[572][1] = '7-8-10'\n",
    "labels[583][1] = '7-8-8'\n",
    "labels[644][1] = '8-9-9'\n",
    "labels[674][1] = '9-9-9'\n",
    "labels[724][1] = '8-9-9'\n",
    "labels[739][1] = '8/8/9'\n",
    "labels[752][1] = '5.5.7'\n",
    "labels[764][1] = '6/7/9'\n",
    "labels[777][1] = '9/9/9'\n",
    "labels[845][1] = '6/8/8'\n",
    "labels[913][1] = '9-9-9'\n",
    "labels[6][1] = '6,5'\n",
    "labels[7][1] = '7,1'\n",
    "labels[538][1] = '1,6'\n",
    "labels[886][1] = '60-68'\n",
    "labels[296][1] = '2.4mm'\n",
    "labels[329][1] = '2.5mm'\n",
    "labels[489][1] = '6.5mm'\n",
    "labels[773][1] = '4mm'\n",
    "labels[185][1] = '4.4mm'\n",
    "labels[186][1] = '3.5mm'\n",
    "labels[297][1] = '2mm'\n",
    "labels[390][1] = '10mm'\n",
    "labels[490][1] = '7mm'\n",
    "labels[586][1] = '4.4mm'\n",
    "labels[587][1] = '2.7'\n",
    "labels[774][1] = '7mm'\n",
    "labels[792][1] = '10mm'\n",
    "labels[3][1] = '50-60mmHg'\n",
    "labels[11][1] = '87mmHg'\n",
    "labels[178][1] = '27mmHg'\n",
    "labels[179][1] = '28mmHg'\n",
    "labels[180][1] = '41mmHg'\n",
    "labels[206][1] = '28mmHg'\n",
    "labels[287][1] = '75mmHg'\n",
    "labels[528][1] = '89mmHG'\n",
    "labels[368][1] = '6,8'\n",
    "labels[465][1] = '7,7'\n",
    "labels[159][1] = '25%'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ddb293-8847-471d-98b6-682a611a00c6",
   "metadata": {},
   "source": [
    "# 4. Structuring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5148f-353c-4ae9-943d-45c4f93c5f16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.1. Breaking down the notes in tokens and regrouping + indexing the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "653e78bc-e247-4adc-a2e0-6febda8ea018",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {}\n",
    "for i in list_of_considered_notes:\n",
    "    texts[i] = notes[2*i+2][2].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d4eb097e-1aed-4298-9edb-060f37474581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in list_of_considered_notes:\n",
    "    clean_text = []\n",
    "    for word in texts[i]:\n",
    "        cleaned_words = remove_punctuation(word)\n",
    "        clean_text.append(cleaned_words)\n",
    "    texts[i] = sum(clean_text, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "411d164e-1f26-4f56-949f-11d2b8496d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attr_to_class = {\"Fraction d'éjection\":\"Contractibilité\",\n",
    "                \"Valeur de la fraction d'éjection en Simson\":\"Contractibilité\",\n",
    "                \"Fraction de raccourcissement\": \"Contractibilité\",\n",
    "                'Fréquence cardiaque : bradycardie':'Fréquence cardiaque',\n",
    "                'Diamètre Artère Pulmonaire Droite distale':'Diamètre Artère Pulmonaire',\n",
    "                'Diamètre Artère Pulmonaire Droite proximale': 'Diamètre Artère Pulmonaire',\n",
    "                'Diamètre Artère Pulmonaire Gauche proximale': 'Diamètre Artère Pulmonaire',\n",
    "                'Diamètre Artère Pulmonaire Principale': 'Diamètre Artère Pulmonaire',\n",
    "                'Diamètre Artère Pulmonaire Droite': 'Diamètre Artère Pulmonaire',\n",
    "                'Diamètre Artère Pulmonaire Gauche': 'Diamètre Artère Pulmonaire',\n",
    "                'diamètre Artère Pulmonaire': 'Diamètre Artère Pulmonaire',\n",
    "                'diamètre Artère Pulmonaire Droite': 'Diamètre Artère Pulmonaire',\n",
    "                'Saturation pulsée en oxygène': 'Saturation en oxygène',\n",
    "                'Valeur Saturation Pulsée en Oxygène': 'Saturation en oxygène',\n",
    "                'saturation veineuse en oxygène': 'Saturation en oxygène',\n",
    "                'Valeur de la Saturation Pulsée en oxygène': 'Saturation en oxygène',\n",
    "                'saturation artérielle en oxygène': 'Saturation en oxygène',\n",
    "                'Objectif cible de Saturation en Oxygène': 'Saturation en oxygène',\n",
    "                'Valeur Fraction inspirée en Oxygène': 'Saturation en oxygène',\n",
    "                'score apgar à 1 minute': 'apgar',\n",
    "                'score apgar à 10 minutes': 'apgar',\n",
    "                'score apgar à 5 minutes': 'apgar',\n",
    "                \"score d'apgar (à une minute et cinq minutes)\": 'apgar',\n",
    "                \"score d'apgar (à une minute, cinq minutes et 10 minutes)\": 'apgar',\n",
    "                'gradient ventricule droit-ventricule gauche artère pulmonaire': 'gradient ventriculaire',\n",
    "                'Gradient ventricule droit-artère pulmonaire': 'gradient ventriculaire',\n",
    "                \"gradient de pression entre l'artère pulmonaire droite et le ventricule droit\":'gradient ventriculaire',\n",
    "                \"gradient de pression entre l'artère pulmonaire gauche et le ventricule droit\": 'gradient ventriculaire',\n",
    "                \"gradient de pression entre le ventricule gauche et l'artère pulmonaire\": 'gradient ventriculaire',\n",
    "                \"gradient de pression entre l'artère pulmonaire principale et le ventricule droit\": 'gradient ventriculaire',\n",
    "                \"Gradient Ventricule Gauche - Ventricule Droit\": 'gradient ventriculaire',\n",
    "                \"Gradient de pic Ventricule Gauche - Ventricule Droit\": 'gradient ventriculaire',\n",
    "                \"Gradient valve pulmonaire\": 'gradient ventriculaire',\n",
    "                \"Gradiant max Valve pulmonaire\": 'gradient ventriculaire',\n",
    "                'Taille Communication Inter Auriculaire': 'Taille CIA/CIV',\n",
    "                'Taille de la Communication Inter Ventriculaire': 'Taille CIA/CIV',\n",
    "                'diamètre de la communication interauriculaire': 'Taille CIA/CIV' \n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "74d4bf11-83f6-4e9f-a683-98721076a089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_to_index = {\"Contractibilité\": 1,\n",
    "                  'Fréquence cardiaque': 2,\n",
    "                  'Diamètre Artère Pulmonaire': 3,\n",
    "                  'Saturation en oxygène': 4,\n",
    "                  'apgar': 5,\n",
    "                  'gradient ventriculaire': 6,\n",
    "                  'Taille CIA/CIV': 7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f5011f21-bd80-4257-b5b7-b28b5489cf9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lines_to_ignore = [5, 26, 116, 121, 131, 144, 160, 316, 341, 356, 357, 379, 397, 405, 406, 418, 419, 449, 450, 522, 530, 537, 544, 546, 547, 564, 565, 573, 574, 584, 585, 605, 606, 607, 608, 645, 646, 648, 675, 676, 704, 725, 726, 740, 741, 753, 754, 765, 766, 778, 779, 795, 801, 846, 847, 887, 914, 915]\n",
    "\n",
    "#I think for 316 it's diameter instead of taille de la CIV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243e56a6-9186-462a-aa07-8f625106a341",
   "metadata": {},
   "source": [
    "## 4.2. Annotating each tokens (labels and tokens alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f1a4c-f9ba-4ddc-8cb2-41fe72a0c95a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = {k:[] for k in list_of_considered_notes}\n",
    "for i in range(len(labels)):\n",
    "    if labels[i][2] in attr_to_class:\n",
    "        classe = attr_to_class[labels[i][2]]\n",
    "        if not (i in lines_to_ignore):\n",
    "            patient_id = int(labels[i][0])\n",
    "            classes[patient_id].append((labels[i][1], classe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c511d349-2e03-48c2-b9fb-26d53ce72ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(labels2)):\n",
    "    classe = labels2[i][2]\n",
    "    patient_id = int(labels2[i][0])\n",
    "    classes[patient_id].append((labels2[i][1], classe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b78744-6d17-4a51-bdbf-bd645d5eb045",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.2.1. Finding some notes where some numerical values are duplicated and associating to each occurance the right class one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d50c47cb-3320-4762-a818-aa4f5274b360",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of doublons\n",
      "2 value= 8 class= 7 pos= [56, 76]\n",
      "43 value= 7 class= 5 pos= [23, 25]\n",
      "43 value= 7 class= 5 pos= [23, 25]\n",
      "52 value= 24 class= 1 pos= [143, 224]\n",
      "52 value= 23 class= 1 pos= [135, 216]\n",
      "69 value= 11 class= 3 pos= [136, 182]\n",
      "69 value= 11 class= 3 pos= [136, 182]\n",
      "69 value= 10 class= 3 pos= [144, 187]\n",
      "69 value= 10 class= 7 pos= [144, 187]\n",
      "70 value= 5.5 class= 3 pos= [100, 106]\n",
      "149 value= 30 class= 6 pos= [108, 195]\n",
      "149 value= 30 class= 6 pos= [108, 195]\n",
      "150 value= 11 class= 7 pos= [19, 136]\n",
      "685 value= 2 class= 3 pos= [30, 117, 145]\n",
      "685 value= 2 class= 3 pos= [30, 117, 145]\n",
      "685 value= 7mm class= 3 pos= [179, 193]\n",
      "704 value= 150 class= 2 pos= [157, 291]\n",
      "704 value= 150 class= 2 pos= [157, 291]\n",
      "1730 value= 120 class= 2 pos= [7, 38]\n",
      "1730 value= 120 class= 2 pos= [7, 38]\n",
      "2700 value= 45 class= 2 pos= [52, 260]\n",
      "2700 value= 45 class= 2 pos= [52, 260]\n",
      "3245 value= 100% class= 4 pos= [86, 309]\n",
      "3292 value= 50 class= 4 pos= [195, 280]\n",
      "3292 value= 50 class= 4 pos= [195, 280]\n",
      "3292 value= 40 class= 4 pos= [333, 343]\n",
      "3293 value= 77 class= 2 pos= [91, 174]\n",
      "3293 value= 77 class= 2 pos= [91, 174]\n",
      "3369 value= 92 class= 4 pos= [65, 107]\n",
      "3369 value= 92 class= 4 pos= [65, 107]\n",
      "3425 value= 14% class= 1 pos= [303, 421]\n",
      "3425 value= 14% class= 1 pos= [303, 421]\n",
      "3463 value= 97% class= 4 pos= [119, 147]\n",
      "3463 value= 97% class= 4 pos= [119, 147]\n"
     ]
    }
   ],
   "source": [
    "pos_classes = {k:np.zeros(len(texts[k]), dtype=int) for k in list_of_considered_notes}\n",
    "print(\"list of doublons\")\n",
    "for i in list_of_considered_notes:\n",
    "    for (value, classe) in classes[i]:\n",
    "        if not value == '9 -9-10': #special case to deal with later\n",
    "            index_class = class_to_index[classe]\n",
    "            index = texts[i].index(value)\n",
    "            indices = [k for k in range(len(texts[i])) if texts[i][k]==value]\n",
    "            if len(indices)> 1:\n",
    "                print(i, 'value=', value, 'class=', index_class, 'pos=', indices)\n",
    "            else:\n",
    "                pos_classes[i][index] = index_class\n",
    "        else:\n",
    "            index_class = class_to_index[classe]\n",
    "            index = texts[i].index('-9-10')\n",
    "            pos_classes[i][index] = index_class\n",
    "            pos_classes[i][index-1] = index_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "532a5296-40d2-409b-a2bf-bd5a1364915f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dealing with numerical values doublons in single notes\n",
    "pos_classes[2][56] = 7\n",
    "pos_classes[43][23] = 5\n",
    "pos_classes[43][25] = 5\n",
    "pos_classes[52][143] = 1\n",
    "pos_classes[52][224] = 1\n",
    "pos_classes[52][135] = 1\n",
    "pos_classes[52][216] = 1\n",
    "pos_classes[69][136] = 3\n",
    "pos_classes[69][182] = 3\n",
    "pos_classes[69][144] = 3\n",
    "pos_classes[69][187] = 7\n",
    "pos_classes[70][106] = 3\n",
    "pos_classes[149][108] = 6\n",
    "pos_classes[149][195] = 6\n",
    "pos_classes[149][22] = 5 #apgar\n",
    "pos_classes[149][24] = 5 #apgar\n",
    "pos_classes[149][26] = 5 #apgar\n",
    "pos_classes[150][19] = 7\n",
    "pos_classes[685][30] = 3\n",
    "pos_classes[685][117] = 3\n",
    "pos_classes[685][193] = 3\n",
    "pos_classes[704][157] = 2\n",
    "pos_classes[704][291] = 2\n",
    "#pos_classes[704][216] = 6\n",
    "pos_classes[1730][7] = 2\n",
    "pos_classes[1730][38] = 2\n",
    "pos_classes[2700][52] = 2\n",
    "pos_classes[2700][260] = 2\n",
    "pos_classes[3245][309] = 4\n",
    "pos_classes[3292][195] = 4\n",
    "pos_classes[3292][280] = 4\n",
    "pos_classes[3292][343] = 4\n",
    "pos_classes[3293][91] = 2\n",
    "pos_classes[3293][174] = 2\n",
    "pos_classes[3369][65] = 4\n",
    "pos_classes[3369][107] = 4\n",
    "pos_classes[3425][303] = 1\n",
    "pos_classes[3425][421] = 1\n",
    "#pos_classes[3425][40] = 6\n",
    "#pos_classes[3425][116] = 6\n",
    "pos_classes[3463][119] = 4\n",
    "pos_classes[3463][147] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9f0855d4-f2c3-4b28-b1af-0295146ab385",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_classes[52]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e68290-5bf8-43ae-a3b5-9749a921205d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.3. Breaking down long notes into smaller notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b8993191-b926-4ec8-b9d0-046864e07b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "minimal_sentence_size = 7\n",
    "for i in list_of_considered_notes:\n",
    "    text = texts[i]\n",
    "    if len(text) < minimal_sentence_size:\n",
    "        continue\n",
    "    class_indexes = pos_classes[i]\n",
    "    #listing the breaking points while avoiding breaks like \"Dr. Fournier.\"\n",
    "    breaks = [-1]\n",
    "    for j in range(len(text)):\n",
    "        if (text[j] == '.') and (j > breaks[-1] + minimal_sentence_size):\n",
    "            breaks.append(j)\n",
    "    if breaks[-1]!= len(text)-1:\n",
    "        breaks.append(len(text)-1)\n",
    "    \n",
    "    #checks if the last sentence has the minimal length\n",
    "    if breaks[-1]-breaks[-2] < minimal_sentence_size:\n",
    "        breaks.pop(-2)\n",
    "        \n",
    "    for j in range(len(breaks)-1):\n",
    "        sample = {'tokens': text[breaks[j]+1: breaks[j+1]+1], \n",
    "                  'classes': class_indexes[breaks[j]+1: breaks[j+1]+1],\n",
    "                  'extracted_from': i}\n",
    "        dataset.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4e3386f6-561f-4ca5-94fc-c917982ff80d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1052"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6c0cef89-45ca-4457-90ab-32bf8691c650",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Écho',\n",
       "  'cardiaque',\n",
       "  '(',\n",
       "  '14/08',\n",
       "  ')',\n",
       "  ':',\n",
       "  'gradient',\n",
       "  'VD-VG',\n",
       "  'AP',\n",
       "  'de',\n",
       "  '50-60mmHg',\n",
       "  '.'],\n",
       " 'classes': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0]),\n",
       " 'extracted_from': 1}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a161a1-14eb-4065-aeb1-1de99a669fe3",
   "metadata": {},
   "source": [
    "## 4.4. Splitting the dataset into training, testing, validating while ensuring equal distribution of classes accross datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c7dd1416-aae3-4671-9fda-f4b4c7b76526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_to_sample = {1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[]}\n",
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    for class_idx in sample['classes']:\n",
    "        if class_idx != 0:\n",
    "            class_to_sample[class_idx].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "6bf80d7b-c881-44ec-860f-3aaaafc3336d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 18, 2: 65, 3: 57, 4: 114, 5: 52, 6: 38, 7: 53}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_sample= {k:np.array(v) for k,v in class_to_sample.items()}\n",
    "{k:len(v) for (k,v) in class_to_sample.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "759c90c7-85c0-4d01-b099-23c838c51ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(mask, display= False):\n",
    "    loss = 0\n",
    "    for i in range(1, 8):\n",
    "        counters = np.zeros(3)\n",
    "        n = 0\n",
    "        for j in mask[class_to_sample[i]]:\n",
    "            if j > -1:\n",
    "                counters[j] += 1\n",
    "                n += 1\n",
    "        probs = counters/n\n",
    "        if display:\n",
    "            print(probs)\n",
    "        kl = .15*np.log(probs[0]/.15) + .15*np.log(probs[1]/.15) + .7*np.log(probs[2]/.7)\n",
    "        kl = kl if n > 0 else -np.inf\n",
    "        loss -= kl\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "678094ba-501e-4e4e-924e-f31b0a0fd4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_arr = categorical(torch.tensor([.15, .15, .7]), len(dataset))\n",
    "#mask_arr = -np.ones(len(dataset),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "655fed39-5a63-4a94-ba0a-9931f3867721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    choice_corresponding_losses = np.empty(3)# gives the corresponding losses for each choice of classification of the sample i\n",
    "    mask_arr[i] = 0\n",
    "    choice_corresponding_losses[0] = compute_loss(mask_arr)\n",
    "    \n",
    "    mask_arr[i] = 1\n",
    "    choice_corresponding_losses[1] = compute_loss(mask_arr)\n",
    "    \n",
    "    mask_arr[i] = 2\n",
    "    choice_corresponding_losses[2] = compute_loss(mask_arr)\n",
    "    \n",
    "    choice = np.argmin(choice_corresponding_losses)\n",
    "    mask_arr[i] = choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ca01f-b78f-4bae-8704-1c8813a2b74b",
   "metadata": {},
   "source": [
    "Now we deal with samples which does not contain annoted numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e16c28c8-3111-4548-8174-d81eac0e0ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "802"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_samples = []\n",
    "for i in range(len(dataset)):\n",
    "    if sum(dataset[i]['classes']) == 0:\n",
    "        empty_samples.append(i)\n",
    "empty_samples = np.array(empty_samples)\n",
    "len(empty_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "df1d947d-93e0-4a5c-8ef7-2ffa049881ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(empty_samples)):\n",
    "    if (i%20==6) or (i%20==12) or (i%20==18):\n",
    "        mask_arr[empty_samples[i]] = 0\n",
    "    \n",
    "    elif (i%20==3) or (i%20==9) or (i%20==15):\n",
    "        mask_arr[empty_samples[i]] = 1\n",
    "    \n",
    "    else:\n",
    "        mask_arr[empty_samples[i]] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "fefd3774-27e5-4a5d-9aed-9d9b506be8ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#with open('mask_array.npy', 'wb') as f:\n",
    "#    np.save(f, mask_arr)\n",
    "    \n",
    "with open('mask_array.npy', 'rb') as f:\n",
    "    mask_arr = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "28b1065c-8b33-4122-ada6-9f6303b3df74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16666667 0.16666667 0.66666667]\n",
      "[0.15384615 0.15384615 0.69230769]\n",
      "[0.15789474 0.14035088 0.70175439]\n",
      "[0.14912281 0.14912281 0.70175439]\n",
      "[0.15384615 0.15384615 0.69230769]\n",
      "[0.15789474 0.15789474 0.68421053]\n",
      "[0.1509434  0.1509434  0.69811321]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.003949408748836987"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(mask_arr, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9b142e33-8772-4e2b-b000-70bc4f0ffc34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = []\n",
    "val_ds = []\n",
    "test_ds = []\n",
    "for i in range(len(dataset)):\n",
    "    if mask_arr[i]==0:\n",
    "        test_ds.append(dataset[i])\n",
    "    elif mask_arr[i]==1:\n",
    "        val_ds.append(dataset[i])\n",
    "    else:\n",
    "        train_ds.append(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b9efddb6-8362-471b-b062-22dd3de5b8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 156 737\n"
     ]
    }
   ],
   "source": [
    "print(len(test_ds), len(val_ds), len(train_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96199943-dec1-461a-83b2-30bb354cea32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.4.1. Saving everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "274b290a-e838-489d-8c5d-1c471f45d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sadcsip/test\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(test_ds, fp)\n",
    " \n",
    "#with open(\"test\", \"rb\") as fp:   # Unpickling\n",
    "#   test_ds = pickle.load(fp)\n",
    "\n",
    "with open(\"sadcsip/val\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(val_ds, fp)\n",
    " \n",
    "#with open(\"val\", \"rb\") as fp:   # Unpickling\n",
    "#   val_ds = pickle.load(fp)\n",
    "\n",
    "with open(\"sadcsip/train\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(train_ds, fp)\n",
    " \n",
    "#with open(\"train\", \"rb\") as fp:   # Unpickling\n",
    "#   train_ds = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689cfa4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.5. Minor post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "2289f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sadcsip/test\", \"rb\") as fp:   # Unpickling\n",
    "    test_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"sadcsip/val\", \"rb\") as fp:   # Unpickling\n",
    "    val_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"sadcsip/train\", \"rb\") as fp:   # Unpickling\n",
    "    train_ds = pickle.load(fp)\n",
    "    \n",
    "edited_test_ds = []\n",
    "edited_val_ds = []\n",
    "edited_train_ds = []\n",
    "\n",
    "for sample in test_ds:\n",
    "    edited_text = []\n",
    "    new_classes = []\n",
    "    for i in range(len(sample['tokens'])):\n",
    "        word = sample['tokens'][i]\n",
    "        edited_word = minor_edit(word).split()\n",
    "        if len(edited_word) > 1:\n",
    "            edited_classes = []\n",
    "            for token in edited_word:\n",
    "                \"\"\"\n",
    "                if re.search(r\"[%\\-\\+A-Za-z/:x\\|><X\\*~«]\", token[0]):\n",
    "                    edited_classes.append(0)\n",
    "                else:\n",
    "                    edited_classes.append(sample['classes'][i])\n",
    "                \"\"\"\n",
    "                if re.search(r\"^(\\d+[\\.,]\\d+|\\d+)$\", token):\n",
    "                    edited_classes.append(sample['classes'][i])\n",
    "                else:\n",
    "                    edited_classes.append(0)\n",
    "\n",
    "            #if sample['classes'][i] != 0:\n",
    "            #    print( word, edited_word, edited_classes, \"extracted:\", sample['extracted_from'])\n",
    "        else:\n",
    "            edited_classes = [sample['classes'][i]]\n",
    "        edited_text.append(edited_word)\n",
    "        new_classes.extend(edited_classes)\n",
    "\n",
    "    edited_sample = {'tokens': sum(edited_text, []), 'classes': new_classes, 'extracted_from': sample['extracted_from']}\n",
    "    edited_test_ds.append(edited_sample)\n",
    "\n",
    "for sample in val_ds:\n",
    "    edited_text = []\n",
    "    new_classes = []\n",
    "    for i in range(len(sample['tokens'])):\n",
    "        word = sample['tokens'][i]\n",
    "        edited_word = minor_edit(word).split()\n",
    "        if len(edited_word) > 1:\n",
    "            edited_classes = []\n",
    "            for token in edited_word:\n",
    "                if re.search(r\"^(\\d+[\\.,]\\d+|\\d+)$\", token):\n",
    "                    edited_classes.append(sample['classes'][i])\n",
    "                else:\n",
    "                    edited_classes.append(0)\n",
    "\n",
    "            #if sample['classes'][i] != 0:\n",
    "            #    print( word, edited_word, edited_classes, \"extracted:\", sample['extracted_from'])\n",
    "        else:\n",
    "            edited_classes = [sample['classes'][i]]\n",
    "        edited_text.append(edited_word)\n",
    "        new_classes.extend(edited_classes)\n",
    "\n",
    "    edited_sample = {'tokens': sum(edited_text, []), 'classes': new_classes, 'extracted_from': sample['extracted_from']}\n",
    "    edited_val_ds.append(edited_sample)\n",
    "    \n",
    "for sample in train_ds:\n",
    "    edited_text = []\n",
    "    new_classes = []\n",
    "    for i in range(len(sample['tokens'])):\n",
    "        word = sample['tokens'][i]\n",
    "        edited_word = minor_edit(word).split()\n",
    "        if len(edited_word) > 1:\n",
    "            edited_classes = []\n",
    "            for token in edited_word:\n",
    "                if re.search(r\"^(\\d+[\\.,]\\d+|\\d+)$\", token):\n",
    "                    edited_classes.append(sample['classes'][i])\n",
    "                else:\n",
    "                    edited_classes.append(0)\n",
    "            #if sample['classes'][i] != 0:\n",
    "            #    print( word, edited_word, edited_classes, \"extracted:\", sample['extracted_from'])\n",
    "        else:\n",
    "            edited_classes = [sample['classes'][i]]\n",
    "        edited_text.append(edited_word)\n",
    "        new_classes.extend(edited_classes)\n",
    "\n",
    "    edited_sample = {'tokens': sum(edited_text, []), 'classes': new_classes, 'extracted_from': sample['extracted_from']}\n",
    "    edited_train_ds.append(edited_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "819c1b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sadcsip/test\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(edited_test_ds, fp)\n",
    " \n",
    "with open(\"sadcsip/val\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(edited_val_ds, fp)\n",
    " \n",
    "with open(\"sadcsip/train\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(edited_train_ds, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "facfa6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Écho',\n",
       " 'cardiaque',\n",
       " '(',\n",
       " '14',\n",
       " '/',\n",
       " '08',\n",
       " ')',\n",
       " ':',\n",
       " 'gradient',\n",
       " 'VD-VG',\n",
       " 'AP',\n",
       " 'de',\n",
       " '50',\n",
       " '-',\n",
       " '60',\n",
       " 'mmHg',\n",
       " '.']"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edited_train_ds[4]['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53816a2e-897c-45ea-9223-f026aa93cb8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5. Build Datasets with customly tokenized numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "84bbcc43-e0b3-438a-9756-a0ce8c69f6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"sadcsip/test\", \"rb\") as fp:   # Unpickling\n",
    "    test_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"sadcsip/val\", \"rb\") as fp:   # Unpickling\n",
    "    val_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"sadcsip/train\", \"rb\") as fp:   # Unpickling\n",
    "    train_ds = pickle.load(fp)\n",
    "    \n",
    "tokenized_test_ds = []\n",
    "tokenized_val_ds = []\n",
    "tokenized_train_ds = []\n",
    "\n",
    "for sample in test_ds:\n",
    "    tokenized_text = []\n",
    "    for word in sample['tokens']:\n",
    "        tokenized_word = custom_tokenizer(word)\n",
    "        tokenized_text.append(tokenized_word)\n",
    "        \n",
    "    tokenized_sample = {'tokens': tokenized_text, 'classes': sample['classes'], 'extracted_from': sample['extracted_from']}\n",
    "    tokenized_test_ds.append(tokenized_sample)\n",
    "\n",
    "for sample in val_ds:\n",
    "    tokenized_text = []\n",
    "    for word in sample['tokens']:\n",
    "        tokenized_word = custom_tokenizer(word)\n",
    "        tokenized_text.append(tokenized_word)\n",
    "        \n",
    "    tokenized_sample = {'tokens': tokenized_text, 'classes': sample['classes'], 'extracted_from': sample['extracted_from']}\n",
    "    tokenized_val_ds.append(tokenized_sample)\n",
    "\n",
    "for sample in train_ds:\n",
    "    tokenized_text = []\n",
    "    for word in sample['tokens']:\n",
    "        tokenized_word = custom_tokenizer(word)\n",
    "        tokenized_text.append(tokenized_word)\n",
    "        \n",
    "    tokenized_sample = {'tokens': tokenized_text, 'classes': sample['classes'], 'extracted_from': sample['extracted_from']}\n",
    "    tokenized_train_ds.append(tokenized_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "94523f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Écho',\n",
       " 'cardiaque',\n",
       " '(',\n",
       " '1D4U',\n",
       " '/',\n",
       " '0D8U',\n",
       " ')',\n",
       " ':',\n",
       " 'gradient',\n",
       " 'VD-VG',\n",
       " 'AP',\n",
       " 'de',\n",
       " '5D0U',\n",
       " '-',\n",
       " '6D0U',\n",
       " 'mmHg',\n",
       " '.']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_ds[4]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "9faad9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sadcsip/tokenized_test\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(tokenized_test_ds, fp)\n",
    " \n",
    "with open(\"sadcsip/tokenized_val\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(tokenized_val_ds, fp)\n",
    " \n",
    "with open(\"sadcsip/tokenized_train\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(tokenized_train_ds, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53294fa6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 6. Build Datasets for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c06d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sadcsip/test\", \"rb\") as fp:   # Unpickling\n",
    "    test_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"sadcsip/val\", \"rb\") as fp:   # Unpickling\n",
    "    val_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"sadcsip/train\", \"rb\") as fp:   # Unpickling\n",
    "    train_ds = pickle.load(fp)\n",
    "    \n",
    "qa_test_ds = []\n",
    "qa_val_ds = []\n",
    "qa_train_ds = []\n",
    "\n",
    "for sample in test_ds:\n",
    "    for i in range(len(sample['tokens'])):\n",
    "        if re.search(r\"^(\\d+[,\\.]\\d+|\\d+\\.?)$\", sample['tokens'][i]):\n",
    "            qa_text = [\"text:\"] + sample['tokens']\n",
    "            qa_text = qa_text + \"\\n A quelle catégorie appartient\".split() + [sample['tokens'][i]]\n",
    "            qa_text = qa_text + \"parmi 1 = Contractibilité, 2 = Fréquence cardiaque, 3 = Diamètre Artère Pulmonaire, \\\n",
    "                  4 = Saturation en oxygène, 5 = apgar, 6 = gradient ventriculaire, 7 = Taille CIA/CIV, 0 = aucune\".split()\n",
    "            \n",
    "            qa_sample = {'text': qa_text, 'label': sample['classes'][i], 'extracted_from': sample['extracted_from']}\n",
    "            qa_test_ds.append(qa_sample)\n",
    "\n",
    "for sample in val_ds:\n",
    "    for i in range(len(sample['tokens'])):\n",
    "        if re.search(r\"^(\\d+[,\\.]\\d+|\\d+\\.?)$\", sample['tokens'][i]):\n",
    "            qa_text = [\"text:\"] + sample['tokens']\n",
    "            qa_text = qa_text + \"\\n A quelle catégorie appartient\".split() + [sample['tokens'][i]]\n",
    "            qa_text = qa_text + \"parmi 1 = Contractibilité, 2 = Fréquence cardiaque, 3 = Diamètre Artère Pulmonaire, \\\n",
    "                  4 = Saturation en oxygène, 5 = apgar, 6 = gradient ventriculaire, 7 = Taille CIA/CIV, 0 = aucune\".split()\n",
    "    \n",
    "            qa_sample = {'text': qa_text, 'label': sample['classes'][i], 'extracted_from': sample['extracted_from']}\n",
    "            qa_val_ds.append(qa_sample)\n",
    "\n",
    "for sample in train_ds:\n",
    "    for i in range(len(sample['tokens'])):\n",
    "        if re.search(r\"^(\\d+[,\\.]\\d+|\\d+\\.?)$\", sample['tokens'][i]):\n",
    "            qa_text = [\"text:\"] + sample['tokens']\n",
    "            qa_text = qa_text + \"\\n A quelle catégorie appartient\".split() + [sample['tokens'][i]]\n",
    "            qa_text = qa_text + \"parmi 1 = Contractibilité, 2 = Fréquence cardiaque, 3 = Diamètre Artère Pulmonaire, \\\n",
    "                  4 = Saturation en oxygène, 5 = apgar, 6 = gradient ventriculaire, 7 = Taille CIA/CIV, 0 = aucune\".split()\n",
    "        \n",
    "            qa_sample = {'text': qa_text, 'label': sample['classes'][i], 'extracted_from': sample['extracted_from']}\n",
    "            qa_train_ds.append(qa_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ada248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sadcsip/qa_test\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(qa_test_ds, fp)\n",
    " \n",
    "with open(\"sadcsip/qa_val\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(qa_val_ds, fp)\n",
    " \n",
    "with open(\"sadcsip/qa_train\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(qa_train_ds, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59615c4-202a-4727-9104-e008dc339edb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 7. Build Blind Dataset (for first paper) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4aff1152-f4ef-4933-ae9c-6ce72b44801a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"sadcsip/test\", \"rb\") as fp:   # Unpickling\n",
    "    test_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"sadcsip/val\", \"rb\") as fp:   # Unpickling\n",
    "    val_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"sadcsip/train\", \"rb\") as fp:   # Unpickling\n",
    "    train_ds = pickle.load(fp)\n",
    "\n",
    "blind_test_ds = []\n",
    "blind_val_ds = []\n",
    "blind_train_ds = []\n",
    "\n",
    "for sample in test_ds:\n",
    "    blind_text = []\n",
    "    for token in sample['tokens']:\n",
    "        #if sample['classes'][i] != 0:\n",
    "        #    blind_sample['tokens'][i] = 'nombre'\n",
    "        blind_text.append(number_masking(token))\n",
    "    blind_sample = {'tokens': blind_text, 'classes': sample['classes'].copy(), 'extracted_from': sample['extracted_from']}\n",
    "    blind_test_ds.append(blind_sample)\n",
    "            \n",
    "for sample in val_ds:\n",
    "    blind_text = []\n",
    "    for token in sample['tokens']:\n",
    "        #if sample['classes'][i] != 0:\n",
    "        #    blind_sample['tokens'][i] = 'nombre'\n",
    "        blind_text.append(number_masking(token))\n",
    "    blind_sample = {'tokens': blind_text, 'classes': sample['classes'].copy(), 'extracted_from': sample['extracted_from']}\n",
    "    blind_val_ds.append(blind_sample)\n",
    "    \n",
    "for sample in train_ds:\n",
    "    blind_text = []\n",
    "    for token in sample['tokens']:\n",
    "        #if sample['classes'][i] != 0:\n",
    "        #    blind_sample['tokens'][i] = 'nombre'\n",
    "        blind_text.append(number_masking(token))\n",
    "    blind_sample = {'tokens': blind_text, 'classes': sample['classes'].copy(), 'extracted_from': sample['extracted_from']}\n",
    "    blind_train_ds.append(blind_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0400a2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Écho',\n",
       " 'cardiaque',\n",
       " '(',\n",
       " 'nombre',\n",
       " '/',\n",
       " 'nombre',\n",
       " ')',\n",
       " ':',\n",
       " 'gradient',\n",
       " 'VD-VG',\n",
       " 'AP',\n",
       " 'de',\n",
       " 'nombre',\n",
       " '-',\n",
       " 'nombre',\n",
       " 'mmHg',\n",
       " '.']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blind_train_ds[4]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5add76c4-0c1d-4b6a-8c05-a0431475b7b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"sadcsip/blind_test\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(blind_test_ds, fp)\n",
    " \n",
    "#with open(\"sadcsip/blind_test\", \"rb\") as fp:   # Unpickling\n",
    "#   blind_test_ds = pickle.load(fp)\n",
    "\n",
    "with open(\"sadcsip/blind_val\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(blind_val_ds, fp)\n",
    " \n",
    "#with open(\"sadcsip/blind_val\", \"rb\") as fp:   # Unpickling\n",
    "#   blind_val_ds = pickle.load(fp)\n",
    "\n",
    "with open(\"sadcsip/blind_train\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(blind_train_ds, fp)\n",
    " \n",
    "#with open(\"sadcsip/blind_train\", \"rb\") as fp:   # Unpickling\n",
    "#   blind_train_ds = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3b5409c8-b92a-46c9-b52b-f286aa3e72f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737 156 159\n"
     ]
    }
   ],
   "source": [
    "print(len(blind_train_ds), len(blind_val_ds), len(blind_test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed78d60",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8. Build Blind Dataset (for second paper) Xval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c3b5485",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_exponents = -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7a0a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sadcsip/test\", \"rb\") as fp:   # Unpickling\n",
    "    test_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"sadcsip/val\", \"rb\") as fp:   # Unpickling\n",
    "    val_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"sadcsip/train\", \"rb\") as fp:   # Unpickling\n",
    "    train_ds = pickle.load(fp)\n",
    "\n",
    "xval_test_ds = []\n",
    "xval_val_ds = []\n",
    "xval_train_ds = []\n",
    "\n",
    "for sample in test_ds:\n",
    "    blind_text = []\n",
    "    h_nums = []\n",
    "    significands = []\n",
    "    exponents = []\n",
    "    for token in sample['tokens']:\n",
    "        t, h = number_blinding(token)\n",
    "        significand, exponent = scientific_notiation(h)\n",
    "        blind_text.append(t)\n",
    "        h_nums.append(h)\n",
    "        significands.append(significand)\n",
    "        exponents.append(exponent - min_exponents)\n",
    "        #if (blind_text[-1] != token) and (blind_text[-1] != 'nombre'):\n",
    "        #    print(token, blind_text[-1], sample['extracted_from'])\n",
    "        #if blind_text[-1] == 'nombre':\n",
    "        #    print(token)\n",
    "\n",
    "    blind_sample = {'tokens': blind_text, 'classes': sample['classes'].copy(),\n",
    "        'h_nums': h_nums, 'significands': significands, 'exponents': exponents, \n",
    "        'extracted_from': sample['extracted_from']}\n",
    "    xval_test_ds.append(blind_sample)\n",
    "            \n",
    "for sample in val_ds:\n",
    "    blind_text = []\n",
    "    h_nums = []\n",
    "    significands = []\n",
    "    exponents = []\n",
    "    for token in sample['tokens']:\n",
    "        t, h = number_blinding(token)\n",
    "        significand, exponent = scientific_notiation(h)\n",
    "        blind_text.append(t)\n",
    "        h_nums.append(h)\n",
    "        significands.append(significand)\n",
    "        exponents.append(exponent - min_exponents)\n",
    "        #if (blind_text[-1] != token) and (blind_text[-1] != 'nombre'):\n",
    "        #    print(token, blind_text[-1], sample['extracted_from'])\n",
    "        #if blind_text[-1] == 'nombre':\n",
    "        #    print(token)\n",
    "\n",
    "    blind_sample = {'tokens': blind_text, 'classes': sample['classes'].copy(),\n",
    "        'h_nums': h_nums, 'significands': significands, 'exponents': exponents, \n",
    "        'extracted_from': sample['extracted_from']}\n",
    "    xval_val_ds.append(blind_sample)\n",
    "    \n",
    "for sample in train_ds:\n",
    "    blind_text = []\n",
    "    h_nums = []\n",
    "    significands = []\n",
    "    exponents = []\n",
    "    for token in sample['tokens']:\n",
    "        t, h = number_blinding(token)\n",
    "        significand, exponent = scientific_notiation(h)\n",
    "        blind_text.append(t)\n",
    "        h_nums.append(h)\n",
    "        significands.append(significand)\n",
    "        exponents.append(exponent - min_exponents)\n",
    "        #if (blind_text[-1] != token) and (blind_text[-1] != 'nombre'):\n",
    "        #    print(token, blind_text[-1], sample['extracted_from'])\n",
    "        #if blind_text[-1] == 'nombre':\n",
    "        #    print(token)\n",
    "\n",
    "    blind_sample = {'tokens': blind_text, 'classes': sample['classes'].copy(),\n",
    "        'h_nums': h_nums, 'significands': significands, 'exponents': exponents, \n",
    "        'extracted_from': sample['extracted_from']}\n",
    "    xval_train_ds.append(blind_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ab103b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Écho',\n",
       "  'cardiaque',\n",
       "  '(',\n",
       "  'NUM',\n",
       "  '/',\n",
       "  'NUM',\n",
       "  ')',\n",
       "  ':',\n",
       "  'gradient',\n",
       "  'VD-VG',\n",
       "  'AP',\n",
       "  'de',\n",
       "  'NUM',\n",
       "  '-',\n",
       "  'NUM',\n",
       "  'mmHg',\n",
       "  '.'],\n",
       " 'classes': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0],\n",
       " 'h_nums': [1, 1, 1, 14.0, 1, 8.0, 1, 1, 1, 1, 1, 1, 50.0, 1, 60.0, 1, 1],\n",
       " 'significands': [1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.4,\n",
       "  1.0,\n",
       "  8.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  5.0,\n",
       "  1.0,\n",
       "  6.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " 'exponents': [4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4],\n",
       " 'extracted_from': 1}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xval_train_ds[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0a03c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sadcsip/xval_test\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(xval_test_ds, fp)\n",
    " \n",
    "with open(\"sadcsip/xval_val\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(xval_val_ds, fp)\n",
    " \n",
    "with open(\"sadcsip/xval_train\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(xval_train_ds, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af1637a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737 156 159\n"
     ]
    }
   ],
   "source": [
    "print(len(xval_train_ds), len(xval_val_ds), len(xval_test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e73bc-8924-40ee-b2cf-dfc7e3cd9c5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 9. Creation of the unlabeled dataset for MLM task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4a14e793-3f2d-4008-99a5-e802baba7811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i in range(2, len(notes), 2):\n",
    "    texts.append(re.sub(r\"(^|\\s)(\\d+)(q|p)\\s(\\d)\", r\"\\1\\2\\3\\4\", notes[i][2]).split())\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    clean_text = []\n",
    "    for word in texts[i]:\n",
    "        cleaned_words = remove_punctuation(word)\n",
    "        clean_text.append(cleaned_words)\n",
    "    texts[i] = sum(clean_text, [])\n",
    "    \n",
    "dataset = []\n",
    "minimal_sentence_size = 10\n",
    "for i in range(len(texts)):\n",
    "    text = texts[i]\n",
    "    if len(text) < minimal_sentence_size:\n",
    "        continue\n",
    "    \n",
    "    #listing the breaking points while avoiding breaks like \"Dr. Fournier.\"\n",
    "    breaks = [-1]\n",
    "    for j in range(len(text)):\n",
    "        if (text[j] == '.') and (j > breaks[-1] + minimal_sentence_size):\n",
    "            breaks.append(j)\n",
    "    if breaks[-1]!= len(text)-1:\n",
    "        breaks.append(len(text)-1)\n",
    "    \n",
    "    #checks if the last sentence has the minimal length\n",
    "    if breaks[-1]-breaks[-2] < minimal_sentence_size:\n",
    "        breaks.pop(-2)\n",
    "    \n",
    "    for j in range(len(breaks)-1):\n",
    "        sample = {'tokens': text[breaks[j]+1: breaks[j+1]+1], \n",
    "                  'extracted_from': i}\n",
    "        #minor post editing\n",
    "        edited_text = []\n",
    "        for word in sample['tokens']:\n",
    "            edited_word = minor_edit(word).split()\n",
    "            #if re.search(r\"«\", word):\n",
    "            #    edited_word = minor_edit(word)\n",
    "            #    print(word, edited_word, sample['extracted_from'])\n",
    "            edited_text.append(edited_word)\n",
    "        \n",
    "        edited_sample = {'tokens': sum(edited_text, []), 'extracted_from': sample['extracted_from']}\n",
    "    \n",
    "        dataset.append(edited_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "812e3992-0885-4c53-b9fe-4e5fb0f9e52d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22238"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "911038a9-c64d-4a69-87cc-514eb78a4a47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand = np.random.rand(len(dataset))\n",
    "mask_arr = (rand < 0.2)\n",
    "mlm_train_ds = []\n",
    "mlm_val_ds = []\n",
    "for i in range(len(dataset)):\n",
    "    if mask_arr[i]:\n",
    "        mlm_val_ds.append(dataset[i])\n",
    "    else:\n",
    "        mlm_train_ds.append(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "feee0eba-f909-440b-8498-e25e1bcc167c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17818 4420\n"
     ]
    }
   ],
   "source": [
    "print(len(mlm_train_ds), len(mlm_val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "63fb0b40-be05-4318-b04c-f65da334ca71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"sadcsip/mlm_val\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(mlm_val_ds, fp)\n",
    " \n",
    "#with open(\"sadcsip/mlm_val\", \"rb\") as fp:   # Unpickling\n",
    "#   mlm_val_ds = pickle.load(fp)\n",
    "\n",
    "with open(\"sadcsip/mlm_train\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(mlm_train_ds, fp)\n",
    " \n",
    "#with open(\"sadcsip/mlm_train\", \"rb\") as fp:   # Unpickling\n",
    "#   mlm_train_ds = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c479b02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 10. Creation of the unlabeled dataset for MLM task Xval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "56aab50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_exponents = -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c9237cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sadcsip/mlm_val\", \"rb\") as fp:   # Unpickling\n",
    "   mlm_val_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"sadcsip/mlm_train\", \"rb\") as fp:   # Unpickling\n",
    "   mlm_train_ds = pickle.load(fp)\n",
    "\n",
    "xval_mlm_val_ds = []\n",
    "xval_mlm_train_ds = []\n",
    "all_exponents = []\n",
    "\n",
    "for sample in mlm_val_ds:\n",
    "    blind_text = []\n",
    "    h_nums = []\n",
    "    significands = []\n",
    "    exponents = []\n",
    "    for token in sample['tokens']:\n",
    "        t, h = number_blinding(token)\n",
    "        significand, exponent = scientific_notiation(h)\n",
    "        blind_text.append(t)\n",
    "        h_nums.append(h)\n",
    "        significands.append(significand)\n",
    "        exponents.append(exponent - min_exponents)\n",
    "        all_exponents.append(exponent)\n",
    "        #if (blind_text[-1] != token) and (blind_text[-1] != 'nombre'):\n",
    "        #    print(token, blind_text[-1], sample['extracted_from'])\n",
    "        #if blind_text[-1] == 'nombre':\n",
    "        #    print(token)\n",
    "\n",
    "    blind_sample = {'tokens': blind_text,\n",
    "        'h_nums': h_nums, 'significands': significands, 'exponents': exponents, \n",
    "        'extracted_from': sample['extracted_from']}\n",
    "    xval_mlm_val_ds.append(blind_sample)\n",
    "            \n",
    "for sample in mlm_train_ds:\n",
    "    blind_text = []\n",
    "    h_nums = []\n",
    "    significands = []\n",
    "    exponents = []\n",
    "    for token in sample['tokens']:\n",
    "        t, h = number_blinding(token)\n",
    "        significand, exponent = scientific_notiation(h)\n",
    "        blind_text.append(t)\n",
    "        h_nums.append(h)\n",
    "        significands.append(significand)\n",
    "        exponents.append(exponent - min_exponents)\n",
    "        all_exponents.append(exponent)\n",
    "        #if (blind_text[-1] != token) and (blind_text[-1] != 'nombre'):\n",
    "        #    print(token, blind_text[-1], sample['extracted_from'])\n",
    "        #if blind_text[-1] == 'nombre':\n",
    "        #    print(token)\n",
    "\n",
    "    blind_sample = {'tokens': blind_text,\n",
    "        'h_nums': h_nums, 'significands': significands, 'exponents': exponents, \n",
    "        'extracted_from': sample['extracted_from']}\n",
    "    xval_mlm_train_ds.append(blind_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5d91ec00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(all_exponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "840010b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sadcsip/xval_mlm_val\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(xval_mlm_val_ds, fp)\n",
    " \n",
    "with open(\"sadcsip/xval_mlm_train\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(xval_mlm_train_ds, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2131b-e8b1-4c16-b0e5-c4b47e0143f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 11. Creation of the unlabeled tokenized dataset for MLM task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2b960683-fdcf-4ec7-8ffa-14d7fbb8498a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " with open(\"sadcsip/mlm_val\", \"rb\") as fp:   # Unpickling\n",
    "    val_ds = pickle.load(fp)\n",
    " \n",
    "with open(\"sadcsip/mlm_train\", \"rb\") as fp:   # Unpickling\n",
    "    train_ds = pickle.load(fp)\n",
    "    \n",
    "mlm_tokenized_val_ds = []\n",
    "mlm_tokenized_train_ds = []\n",
    "mlm_tokenized_global_ds = []\n",
    "\n",
    "for sample in val_ds:\n",
    "    tokenized_sample = {k:v for (k,v) in sample.items()}\n",
    "    tokenized_sample['tokens'] = [custom_tokenizer(word) for word in sample['tokens']]\n",
    "    mlm_tokenized_val_ds.append(tokenized_sample)\n",
    "    mlm_tokenized_global_ds.append(tokenized_sample)\n",
    "\n",
    "for sample in train_ds:\n",
    "    tokenized_sample = {k:v for (k,v) in sample.items()}\n",
    "    tokenized_sample['tokens'] = [custom_tokenizer(word).split() for word in sample['tokens']]\n",
    "    mlm_tokenized_train_ds.append(tokenized_sample)\n",
    "    mlm_tokenized_global_ds.append(tokenized_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b205ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sadcsip/mlm_tokenized_val\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(mlm_tokenized_val_ds, fp)\n",
    " \n",
    "with open(\"sadcsip/mlm_tokenized_train\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(mlm_tokenized_train_ds, fp)\n",
    "\n",
    "with open(\"sadcsip/mlm_tokenized_global\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(mlm_tokenized_global_ds, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2d8748c9-107d-4ed9-89db-4c304a007ade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22238"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlm_tokenized_global_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "796a56d1-333f-4445-ab59-edc299db6b47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17818 4420\n"
     ]
    }
   ],
   "source": [
    "print(len(mlm_tokenized_train_ds), len(mlm_tokenized_val_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e24798",
   "metadata": {},
   "source": [
    "# 12. Creation of the ComNumDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0282650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "len_dataset = 200000\n",
    "candidate_numbers = [k for k in range(len_dataset)]\n",
    "dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95d91bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len_dataset//4):\n",
    "    selected_numbers = random.sample(candidate_numbers, k=2)\n",
    "    text = aux(str(selected_numbers[0]/1000)) + ' est supérieur à ' + aux(str(selected_numbers[1]/1000)) + '.'\n",
    "    label = selected_numbers[0] > selected_numbers[1]\n",
    "    dataset.append({'text': text, 'label': label})\n",
    "\n",
    "    for number in selected_numbers:\n",
    "        candidate_numbers.remove(number)\n",
    "\n",
    "    selected_numbers = random.sample(candidate_numbers, k=2)\n",
    "    text = aux(str(selected_numbers[0]/1000)) + ' est inférieur à ' + aux(str(selected_numbers[1]/1000)) + '.'\n",
    "    label = selected_numbers[0] < selected_numbers[1]\n",
    "    dataset.append({'text': text, 'label': label})\n",
    "    for number in selected_numbers:\n",
    "        candidate_numbers.remove(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "198d0aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ceb87bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = np.random.rand(len(dataset))\n",
    "mask_arr = (rand < 0.2)\n",
    "mlm_train_ds = []\n",
    "mlm_val_ds = []\n",
    "for i in range(len(dataset)):\n",
    "    if mask_arr[i]:\n",
    "        mlm_val_ds.append(dataset[i])\n",
    "    else:\n",
    "        mlm_train_ds.append(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86335e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80096 19904\n"
     ]
    }
   ],
   "source": [
    "print(len(mlm_train_ds), len(mlm_val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b83a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mlm_val_3\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(mlm_val_ds, fp)\n",
    " \n",
    "with open(\"mlm_train_3\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(mlm_train_ds, fp)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13c894-6daa-4d69-8b3d-cb5ba44e0896",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 13. Tokenized version of E3C dataset\n",
    "\n",
    "Download the french corpus of E3C dataset [here](https://github.com/hltfbk/E3C-Corpus/tree/v2.0.0) prior, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fc64204-b26c-4467-bcd7-fc56ec60b61b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:00<00:00, 245.22it/s]\n",
      "100%|██████████| 168/168 [00:00<00:00, 256.64it/s]\n",
      "100%|██████████| 25740/25740 [24:05<00:00, 17.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "directories = ['layer1', 'layer2', 'layer3']\n",
    "dataset = []\n",
    "for directory in directories:\n",
    "    path = os.path.join('E3C-Corpus/data_collection/French', directory)\n",
    "    for filename in tqdm(os.listdir(path)):\n",
    "        f = os.path.join(path, filename)\n",
    "        file = open(f)\n",
    "        data = json.load(file)\n",
    "        raw_text = re.sub(\"\\.(\\.|\\s)*\\.\", '', data[\"text\"])\n",
    "        raw_text = re.sub(r\"(\\D{3})(\\d{1,2})\\s(\\d{3})\\s(\\d{3})(\\D{3})\", r\"\\1\\2\\3\\4\\5\", raw_text) #detecting 7+ digits numbers with whitespace\n",
    "        raw_text = re.sub(r\"(\\D{3})(\\d{1,3})\\s(\\d{3})(\\D{3})\", r\"\\1\\2\\3\\4\", raw_text) # detecting 4+ digits numbers with whitespace\n",
    "        raw_text = raw_text.split()\n",
    "        preprocessed_text = []\n",
    "        for word in raw_text:\n",
    "            cleaned_words = remove_punctuation(word)\n",
    "            tokenized_cleaned_words = []\n",
    "            try:\n",
    "                for cleaned_word in cleaned_words:\n",
    "                    tokenized_cleaned_words.append(custom_tokenizer(cleaned_word).split())\n",
    "                    #if len(tokenized_cleaned_words[-1])>1:\n",
    "                    #    print(\"'\", cleaned_word, \"'\", tokenized_cleaned_words[-1], \"extracted:\", f)\n",
    "                cleaned_words = sum(tokenized_cleaned_words, [])\n",
    "\n",
    "            except IndexError:\n",
    "                print(cleaned_words, f)\n",
    "            #except Exception:\n",
    "            #    print(cleaned_words, f)\n",
    "            preprocessed_text.append(cleaned_words)\n",
    "        dataset.append(sum(preprocessed_text, []))\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d57fe6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the whole dataset first\n",
    "e3c_mlm_tokenized_global_ds = []\n",
    "minimal_sentence_size = 390\n",
    "for text in dataset:\n",
    "    #listing the breaking points while avoiding breaks like \"Dr. Fournier.\"\n",
    "    breaks = [-1]\n",
    "    for j in range(len(text)):\n",
    "        if (text[j] == '.') and (j > breaks[-1] + minimal_sentence_size):\n",
    "            breaks.append(j)\n",
    "    if breaks[-1]!= len(text)-1:\n",
    "        breaks.append(len(text)-1)\n",
    "\n",
    "    #checks if the last sentence has the minimal length\n",
    "    if breaks[-1]-breaks[-2] < minimal_sentence_size:\n",
    "        breaks.pop(-2)\n",
    "        \n",
    "    for j in range(len(breaks)-1):\n",
    "        e3c_mlm_tokenized_global_ds.append({'tokens': text[breaks[j]+1: breaks[j+1]+1]})\n",
    "\n",
    "with open(\"e3c_mlm_tokenized_global\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(e3c_mlm_tokenized_global_ds, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a94772a-f6b3-423e-8463-962922c84542",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = np.random.rand(len(e3c_mlm_tokenized_global_ds))\n",
    "mask_arr = (rand < 0.2)\n",
    "e3c_mlm_tokenized_train_ds = []\n",
    "e3c_mlm_tokenized_val_ds = []\n",
    "for i in range(len(e3c_mlm_tokenized_global_ds)):\n",
    "    if mask_arr[i]:\n",
    "        e3c_mlm_tokenized_val_ds.append(e3c_mlm_tokenized_global_ds[i])\n",
    "    else:\n",
    "        e3c_mlm_tokenized_train_ds.append(e3c_mlm_tokenized_global_ds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de545738",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"e3c_mlm_tokenized_val\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(e3c_mlm_tokenized_val_ds, fp)\n",
    " \n",
    "with open(\"e3c_mlm_tokenized_train\", \"wb\") as fp:   #Pickling\n",
    "   pickle.dump(e3c_mlm_tokenized_train_ds, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7219127d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132950 33041\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(len(e3c_mlm_tokenized_train_ds), len(e3c_mlm_tokenized_val_ds), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a820c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
